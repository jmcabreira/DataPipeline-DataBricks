{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81574298-f441-4f41-aee8-7275eec42cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# # Loading Data To Gold Zone \n",
    "\n",
    "**This Notebook:**\n",
    "* Load data to Golg Zone of the Data Lake House\n",
    "* Star Schekma and One Big Table Modeling\n",
    "* Creates **`IDENTITY`** column in Databricks delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecd3c5b5-8d45-4392-a9f5-73a260112d4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.0 Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "97e69ae5-8acb-447a-ac48-de146404d808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting dbldatagen\n  Downloading dbldatagen-0.4.0.post1-py3-none-any.whl (122 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.8/122.8 kB 3.6 MB/s eta 0:00:00\nInstalling collected packages: dbldatagen\nSuccessfully installed dbldatagen-0.4.0.post1\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%run \"/Users/cabreirajm@gmail.com/DataPipelineCabreira/Helpers/data_generator\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66d62ab8-f059-4ecd-afa5-bb95afed9b19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.0 Create `Gold Zone` Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dd56966-dd4a-4a81-ac81-18e21a99a82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a35b5d6-0dfc-46a6-a536-493cd285a6c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.0 `Sales Star Schema` Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e59efc0-3761-4546-9e25-fc1431df0e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aiming to optimize queries in large datasets, we can use a dimensional model. \n",
    "We will use Ralph Kimball data warehouse principles and build a Star Schema model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "352565aa-3b9a-435b-8621-413a1c0e091a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### `Dimensional Tables`\n",
    "- **dim_calendar** - Dimension with date information\n",
    "- **dim_cod** - Dimensions with codes  - Low cardinality Dimensions (Junk Dimension): \n",
    "  - **user_origin** - API vs. Files\n",
    "  - **access_from** - mobile vs. computer\n",
    "  - **payment_method** - Pix vs. Boleto vs. Cartão\n",
    "  - **percent_discount** - 5% vs. 10% vs. 15%\n",
    "- **dim_courses** - Dimensão responsável por armazenar as informações de Curso.\n",
    "- **dim_user** - Dimensão responsável por armazenar as informações de Alunos.\n",
    "\n",
    "\n",
    "All tables will have a **Surrogate Key (SK)** column that will be creeated with the **`<col_name> BIGINT GENERATED ALWAYS AS IDENTITY`** command. Spark will populate this column in execution time with an incremental value (incremental(1,1). )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20a9aec8-5dc9-4968-84d6-d3e8e50b8931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.1 `Sale Dimensions`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5dfd8be-7dcf-4a62-b7a3-3ff46861293f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql( \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.dim_calendar(\n",
    "        sk_tempo BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "        date DATE,\n",
    "        year INT, \n",
    "        month STRING,\n",
    "        month_year INT,\n",
    "        day_week_int INT, \n",
    "        day_week STRING,\n",
    "        fl_day_week BOOLEAN,\n",
    "        day_month INT,\n",
    "        fl_last_month_day INT,\n",
    "        day_year INT,\n",
    "        week_year INT,\n",
    "        bimonthly INT,\n",
    "        quarter INT, \n",
    "        semester INT, \n",
    "        dt_load TIMESTAMP\n",
    "    )  \n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.dim_cod (\n",
    "        sk_cod BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "        user_origin STRING,\n",
    "        access_from STRING,\n",
    "        payment_method STRING,\n",
    "        percent_discount STRING,\n",
    "        dt_load TIMESTAMP\n",
    "    )  \n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.dim_course(\n",
    "    sk_course BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    course_uuid STRING,\n",
    "    course_name STRING, \n",
    "    course_level STRING,\n",
    "    cource_price DECIMAL(9,2),\n",
    "    dt_carga TIMESTAMP\n",
    "\n",
    "    )          \n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.dim_user(\n",
    "    sk_user BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    user_uuid STRING,\n",
    "    name_user STRING,\n",
    "    user_email STRING,\n",
    "    user_age INT, \n",
    "    user_gender STRING,\n",
    "    user_state STRING,\n",
    "    user_profession STRING,\n",
    "    company STRING,\n",
    "    dt_load\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a15c2fd-cc5d-4570-ab53-41e552ef64cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.2 `Calendar Dimension`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "203a1c20-ea0b-4d19-91aa-02052f620db0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The view **`vw_dim_tempo`**:\n",
    "* Starts date : **01/06/2024** \n",
    "* End date: **31/12/2025**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f025b7ab-3dc9-442b-b865-ba83670b7aa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-101296273828189>, line 6\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124m         \u001B[39m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;124m  with date as (\u001B[39m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;124m    select\u001B[39m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;124m      explode(\u001B[39m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;124m        sequence(\u001B[39m\n",
       "\u001B[0;32m----> 6\u001B[0m \u001B[38;5;124m          to_date(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_inicio\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m),\u001B[39m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;124m          to_date(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_fim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m),\u001B[39m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;124m          interval 1 day\u001B[39m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;124m        )\u001B[39m\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;124m      ) as data\u001B[39m\n",
       "\u001B[1;32m     11\u001B[0m \u001B[38;5;124m  )\u001B[39m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;124m  select\u001B[39m\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;124m    data,\u001B[39m\n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;124m    year(data) AS year,\u001B[39m\n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;124m    to_csv(\u001B[39m\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;124m      named_struct(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, data),\u001B[39m\n",
       "\u001B[1;32m     17\u001B[0m \u001B[38;5;124m      map(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdateFormat\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMMMM\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocale\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPT\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m)\u001B[39m\n",
       "\u001B[1;32m     18\u001B[0m \u001B[38;5;124m    ) AS mes,\u001B[39m\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;124m    month(data) as month_year,\u001B[39m\n",
       "\u001B[1;32m     20\u001B[0m \u001B[38;5;124m    dayofweek(data) AS day_week_int,\u001B[39m\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;124m    to_csv(\u001B[39m\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;124m      named_struct(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, date),\u001B[39m\n",
       "\u001B[1;32m     23\u001B[0m \u001B[38;5;124m      map(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdateFormat\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEEEE\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocale\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPT\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m)\u001B[39m\n",
       "\u001B[1;32m     24\u001B[0m \u001B[38;5;124m    ) AS day_week,\u001B[39m\n",
       "\u001B[1;32m     25\u001B[0m \u001B[38;5;124m    case\u001B[39m\n",
       "\u001B[1;32m     26\u001B[0m \u001B[38;5;124m      when weekday(data) < 5 then True\u001B[39m\n",
       "\u001B[1;32m     27\u001B[0m \u001B[38;5;124m      else False\u001B[39m\n",
       "\u001B[1;32m     28\u001B[0m \u001B[38;5;124m    end as fl_dia_semana,\u001B[39m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;124m    dayofmonth(data) as day_month,\u001B[39m\n",
       "\u001B[1;32m     30\u001B[0m \u001B[38;5;124m    case\u001B[39m\n",
       "\u001B[1;32m     31\u001B[0m \u001B[38;5;124m      when data = last_day(data) then True\u001B[39m\n",
       "\u001B[1;32m     32\u001B[0m \u001B[38;5;124m      else False\u001B[39m\n",
       "\u001B[1;32m     33\u001B[0m \u001B[38;5;124m    end as fl_ultimo_dia_mes,\u001B[39m\n",
       "\u001B[1;32m     34\u001B[0m \u001B[38;5;124m    dayofyear(data) as dia_ano,\u001B[39m\n",
       "\u001B[1;32m     35\u001B[0m \u001B[38;5;124m    weekofyear(data) as week_year,\u001B[39m\n",
       "\u001B[1;32m     36\u001B[0m \u001B[38;5;124m    case\u001B[39m\n",
       "\u001B[1;32m     37\u001B[0m \u001B[38;5;124m      when month(data) in (1, 2) then 1\u001B[39m\n",
       "\u001B[1;32m     38\u001B[0m \u001B[38;5;124m      when month(data) in (3, 4) then 2\u001B[39m\n",
       "\u001B[1;32m     39\u001B[0m \u001B[38;5;124m      when month(data) in (5, 6) then 3\u001B[39m\n",
       "\u001B[1;32m     40\u001B[0m \u001B[38;5;124m      when month(data) in (7, 8) then 4\u001B[39m\n",
       "\u001B[1;32m     41\u001B[0m \u001B[38;5;124m      when month(data) in (9, 10) then 5\u001B[39m\n",
       "\u001B[1;32m     42\u001B[0m \u001B[38;5;124m      when month(data) in (11, 12) then 6\u001B[39m\n",
       "\u001B[1;32m     43\u001B[0m \u001B[38;5;124m    end as bimestre,\u001B[39m\n",
       "\u001B[1;32m     44\u001B[0m \u001B[38;5;124m    case\u001B[39m\n",
       "\u001B[1;32m     45\u001B[0m \u001B[38;5;124m      when month(data) in (1, 2, 3) then 1\u001B[39m\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;124m      when month(data) in (4, 5, 6) then 2\u001B[39m\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;124m      when month(data) in (7, 8, 9) then 3\u001B[39m\n",
       "\u001B[1;32m     48\u001B[0m \u001B[38;5;124m      when month(data) in (10, 11, 12) then 4\u001B[39m\n",
       "\u001B[1;32m     49\u001B[0m \u001B[38;5;124m    end as trimestre,\u001B[39m\n",
       "\u001B[1;32m     50\u001B[0m \u001B[38;5;124m    case\u001B[39m\n",
       "\u001B[1;32m     51\u001B[0m \u001B[38;5;124m      when month(data) in (1, 2, 3, 4, 5, 6) then 1\u001B[39m\n",
       "\u001B[1;32m     52\u001B[0m \u001B[38;5;124m      when month(data) in (7, 8, 9, 10, 11, 12) then 2\u001B[39m\n",
       "\u001B[1;32m     53\u001B[0m \u001B[38;5;124m    end as semestre\u001B[39m\n",
       "\u001B[1;32m     54\u001B[0m \u001B[38;5;124m  from\u001B[39m\n",
       "\u001B[1;32m     55\u001B[0m \u001B[38;5;124m    datas\u001B[39m\n",
       "\u001B[1;32m     56\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateOrReplaceTempView(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvw_dim_tempo\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m     58\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSELECT * FROM vw_dim_tempo LIMIT 5\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mdisplay()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'data_inicio' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-101296273828189>, line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124m         \u001B[39m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124m  with date as (\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124m    select\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124m      explode(\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124m        sequence(\u001B[39m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;124m          to_date(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_inicio\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m),\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124m          to_date(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_fim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m),\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124m          interval 1 day\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124m        )\u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124m      ) as data\u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124m  )\u001B[39m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124m  select\u001B[39m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;124m    data,\u001B[39m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;124m    year(data) AS year,\u001B[39m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124m    to_csv(\u001B[39m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124m      named_struct(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, data),\u001B[39m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124m      map(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdateFormat\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMMMM\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocale\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPT\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m)\u001B[39m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124m    ) AS mes,\u001B[39m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;124m    month(data) as month_year,\u001B[39m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;124m    dayofweek(data) AS day_week_int,\u001B[39m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;124m    to_csv(\u001B[39m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;124m      named_struct(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, date),\u001B[39m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;124m      map(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdateFormat\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEEEE\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocale\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPT\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m)\u001B[39m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;124m    ) AS day_week,\u001B[39m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;124m    case\u001B[39m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;124m      when weekday(data) < 5 then True\u001B[39m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;124m      else False\u001B[39m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;124m    end as fl_dia_semana,\u001B[39m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;124m    dayofmonth(data) as day_month,\u001B[39m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;124m    case\u001B[39m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;124m      when data = last_day(data) then True\u001B[39m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;124m      else False\u001B[39m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;124m    end as fl_ultimo_dia_mes,\u001B[39m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;124m    dayofyear(data) as dia_ano,\u001B[39m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;124m    weekofyear(data) as week_year,\u001B[39m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;124m    case\u001B[39m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;124m      when month(data) in (1, 2) then 1\u001B[39m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;124m      when month(data) in (3, 4) then 2\u001B[39m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;124m      when month(data) in (5, 6) then 3\u001B[39m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;124m      when month(data) in (7, 8) then 4\u001B[39m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;124m      when month(data) in (9, 10) then 5\u001B[39m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;124m      when month(data) in (11, 12) then 6\u001B[39m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;124m    end as bimestre,\u001B[39m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;124m    case\u001B[39m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;124m      when month(data) in (1, 2, 3) then 1\u001B[39m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;124m      when month(data) in (4, 5, 6) then 2\u001B[39m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;124m      when month(data) in (7, 8, 9) then 3\u001B[39m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;124m      when month(data) in (10, 11, 12) then 4\u001B[39m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124m    end as trimestre,\u001B[39m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;124m    case\u001B[39m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;124m      when month(data) in (1, 2, 3, 4, 5, 6) then 1\u001B[39m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;124m      when month(data) in (7, 8, 9, 10, 11, 12) then 2\u001B[39m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;124m    end as semestre\u001B[39m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;124m  from\u001B[39m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;124m    datas\u001B[39m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateOrReplaceTempView(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvw_dim_tempo\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     58\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSELECT * FROM vw_dim_tempo LIMIT 5\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mdisplay()\n\n\u001B[0;31mNameError\u001B[0m: name 'data_inicio' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'data_inicio' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "spark.sql(f\"\"\"         \n",
    "  with date as (\n",
    "    select\n",
    "      explode(\n",
    "        sequence(\n",
    "          to_date('{data_inicio}'),\n",
    "          to_date('{data_fim}'),\n",
    "          interval 1 day\n",
    "        )\n",
    "      ) as data\n",
    "  )\n",
    "  select\n",
    "    data,\n",
    "    year(data) AS year,\n",
    "    to_csv(\n",
    "      named_struct('date', data),\n",
    "      map('dateFormat', 'MMMM', 'locale', 'PT')\n",
    "    ) AS mes,\n",
    "    month(data) as month_year,\n",
    "    dayofweek(data) AS day_week_int,\n",
    "    to_csv(\n",
    "      named_struct('date', date),\n",
    "      map('dateFormat', 'EEEE', 'locale', 'PT')\n",
    "    ) AS day_week,\n",
    "    case\n",
    "      when weekday(data) < 5 then True\n",
    "      else False\n",
    "    end as fl_dia_semana,\n",
    "    dayofmonth(data) as day_month,\n",
    "    case\n",
    "      when data = last_day(data) then True\n",
    "      else False\n",
    "    end as fl_ultimo_dia_mes,\n",
    "    dayofyear(data) as dia_ano,\n",
    "    weekofyear(data) as week_year,\n",
    "    case\n",
    "      when month(data) in (1, 2) then 1\n",
    "      when month(data) in (3, 4) then 2\n",
    "      when month(data) in (5, 6) then 3\n",
    "      when month(data) in (7, 8) then 4\n",
    "      when month(data) in (9, 10) then 5\n",
    "      when month(data) in (11, 12) then 6\n",
    "    end as bimestre,\n",
    "    case\n",
    "      when month(data) in (1, 2, 3) then 1\n",
    "      when month(data) in (4, 5, 6) then 2\n",
    "      when month(data) in (7, 8, 9) then 3\n",
    "      when month(data) in (10, 11, 12) then 4\n",
    "    end as trimestre,\n",
    "    case\n",
    "      when month(data) in (1, 2, 3, 4, 5, 6) then 1\n",
    "      when month(data) in (7, 8, 9, 10, 11, 12) then 2\n",
    "    end as semestre\n",
    "  from\n",
    "    datas\n",
    "\"\"\").createOrReplaceTempView('vw_dim_tempo')\n",
    "\n",
    "spark.sql('SELECT * FROM vw_dim_tempo LIMIT 5').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de37460-82c3-4ae0-b0b9-4844de4c6619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date</th><th>year</th><th>month</th></tr></thead><tbody><tr><td>2024-06-01</td><td>2024</td><td>Junho</td></tr><tr><td>2024-06-02</td><td>2024</td><td>Junho</td></tr><tr><td>2024-06-03</td><td>2024</td><td>Junho</td></tr><tr><td>2024-06-04</td><td>2024</td><td>Junho</td></tr><tr><td>2024-06-05</td><td>2024</td><td>Junho</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-06-01",
         2024,
         "Junho"
        ],
        [
         "2024-06-02",
         2024,
         "Junho"
        ],
        [
         "2024-06-03",
         2024,
         "Junho"
        ],
        [
         "2024-06-04",
         2024,
         "Junho"
        ],
        [
         "2024-06-05",
         2024,
         "Junho"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode,sequence,to_date \n",
    "\n",
    "start_date = \"2024-06-01\"\n",
    "end_date = \"2025-12-31\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "      with date As ( \n",
    "      select\n",
    "      explode( \n",
    "        sequence(\n",
    "          to_date('{start_date}'),\n",
    "          to_date('{end_date}'),\n",
    "          interval 1 day\n",
    "         )) as Date )\n",
    "      select \n",
    "        Date, \n",
    "        year(Date) AS year,\n",
    "        to_csv(\n",
    "        named_struct('date', Date),\n",
    "        map('dateFormat', 'MMMM', 'locale', 'PT')\n",
    "      ) AS month\n",
    "      from date \n",
    "\n",
    "         --select * from date\n",
    "\"\"\").createOrReplaceTempView('test_view')\n",
    "\n",
    "spark.sql('SELECT * FROM test_view LIMIT 5').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a67959ea-1230-4ac8-b805-f71f391cea92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"         \n",
    "  with datas as (\n",
    "    select\n",
    "      explode(\n",
    "        sequence(\n",
    "          to_date('{data_inicio}'),\n",
    "          to_date('{data_fim}'),\n",
    "          interval 1 day\n",
    "        )\n",
    "      ) as data\n",
    "  )\n",
    "  select\n",
    "    data,\n",
    "    year(data) AS ano,\n",
    "    to_csv(\n",
    "      named_struct('date', data),\n",
    "      map('dateFormat', 'MMMM', 'locale', 'PT')\n",
    "    ) AS mes,\n",
    "    month(data) as mes_ano,\n",
    "    dayofweek(data) AS dia_semana_int,\n",
    "    to_csv(\n",
    "      named_struct('date', data),\n",
    "      map('dateFormat', 'EEEE', 'locale', 'PT')\n",
    "    ) AS dia_semana,\n",
    "    case\n",
    "      when weekday(data) < 5 then True\n",
    "      else False\n",
    "    end as fl_dia_semana,\n",
    "    dayofmonth(data) as dia_mes,\n",
    "    case\n",
    "      when data = last_day(data) then True\n",
    "      else False\n",
    "    end as fl_ultimo_dia_mes,\n",
    "    dayofyear(data) as dia_ano,\n",
    "    weekofyear(data) as semana_ano,\n",
    "    case\n",
    "      when month(data) in (1, 2) then 1\n",
    "      when month(data) in (3, 4) then 2\n",
    "      when month(data) in (5, 6) then 3\n",
    "      when month(data) in (7, 8) then 4\n",
    "      when month(data) in (9, 10) then 5\n",
    "      when month(data) in (11, 12) then 6\n",
    "    end as bimestre,\n",
    "    case\n",
    "      when month(data) in (1, 2, 3) then 1\n",
    "      when month(data) in (4, 5, 6) then 2\n",
    "      when month(data) in (7, 8, 9) then 3\n",
    "      when month(data) in (10, 11, 12) then 4\n",
    "    end as trimestre,\n",
    "    case\n",
    "      when month(data) in (1, 2, 3, 4, 5, 6) then 1\n",
    "      when month(data) in (7, 8, 9, 10, 11, 12) then 2\n",
    "    end as semestre\n",
    "  from\n",
    "    datas\n",
    "\"\"\").createOrReplaceTempView('vw_dim_tempo')\n",
    "\n",
    "spark.sql('SELECT * FROM vw_dim_tempo LIMIT 5').display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Load_Gold_Zone",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
