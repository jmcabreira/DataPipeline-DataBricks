{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ca48b39-6254-44fe-8d5f-fe6ef60d1155",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Loading Data to Silver Zone\n",
    "\n",
    "This Notebook:\n",
    "* We will iIngest data from **Bronze Zone** to **Silver Zone** using spark\n",
    "* We will use Spark Structured Streaming with **`trigger(availableNow=True)`** for batch loading\n",
    "* We will do a **load control** of the batch processes through Structured Streaming **checkpoint**\n",
    "* We will use **`awaitTermination()`**  ethod to transform the streaming queries in a synchronous process\n",
    "* We will Combine spark and sql in order to do the data load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc4cbf26-23cf-4ff9-80a3-8302c1ab5505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.0 Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c3859f71-b7e2-4b25-8028-6301486d5453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/cabreirajm@gmail.com/DataPipelineCabreira/Helpers/data_generator\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "052ad713-ac0b-4051-8b6f-4b247e37cb05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.0 Create `Silver Zone` Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2e899e0-caae-458f-9ab1-ee338ec3c532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b1f652a-4c5d-41ec-adb2-924dfaa4602a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.0 Businesse Requirements for Silver Zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db2d40a5-b072-4525-9475-36d25b09840c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. The ingestion need to be done in batch in order to avoid extra costs \n",
    "    * Even though the API data is stored in streaming in landing zone \n",
    "2. API and Batch Data need to be stored in the same table \n",
    "3. Each table will have an uuid column with a hash to identify each register\n",
    "4. We have to garantee the correct data type of all column \n",
    "5. We need to create better column names for each table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56c5aab8-5a36-48e0-8e79-cd3923a9877d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.0 Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "510ba2f5-3b40-4c6d-982c-1197cc888852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.1 Courses Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67ed5195-477b-4595-b2e7-08aa947aec5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The table `tb_courses` is a **domain table** and we will store all the available courses information ( the product ). Its information will be added manually.\n",
    "* We will use the **md5()** function to create the **curso_uuuid** column by the course name \n",
    "* The column **data_carga** : Contains the processing date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b41fd273-e9ba-4837-a55b-e9511c6a330d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS silver.tb_curso \n",
    "    AS\n",
    "        SELECT \n",
    "            md5('Construindo o seu Primeiro Pipeline de Dados com o Databricks') AS curso_uuid,\n",
    "            'Construindo o seu Primeiro Pipeline de Dados com o Databricks' AS nome_curso,\n",
    "            'beginner' AS nivel_curso,\n",
    "            589.90 AS valor_curso,\n",
    "            getdate() AS data_carga\n",
    "\n",
    "        UNION \n",
    "\n",
    "        SELECT \n",
    "            md5('Do Primeiro Pipeline ao Data Lakehouse com o Databricks') AS curso_uuid,\n",
    "            'Do Primeiro Pipeline ao Data Lakehouse com o Databricks' AS nome_curso,\n",
    "            'intermediate' AS nivel_curso,\n",
    "            659.90 AS valor_curso,\n",
    "            getdate() AS data_carga\n",
    "\n",
    "        UNION \n",
    "\n",
    "        SELECT \n",
    "            md5('Construindo Pipelines de Dados usando o Spark Structured Streaming') AS curso_uuid,\n",
    "            'Construindo Pipelines de Dados usando o Spark Structured Streaming' AS nome_curso,\n",
    "            'intermediate' AS nivel_curso,\n",
    "            549.90 AS valor_curso,\n",
    "            getdate() AS data_carga\n",
    "\"\"\")\n",
    "\n",
    "spark.sql('SELECT * FROM silver.tb_curso').display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Load_Silver_Layer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
