{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81574298-f441-4f41-aee8-7275eec42cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# # Loading Data To Gold Zone \n",
    "\n",
    "**This Notebook:**\n",
    "* Load data to Golg Zone of the Data Lake House\n",
    "* Star Schekma and One Big Table Modeling\n",
    "* Creates **`IDENTITY`** column in Databricks delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecd3c5b5-8d45-4392-a9f5-73a260112d4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.0 Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "97e69ae5-8acb-447a-ac48-de146404d808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting dbldatagen\n  Downloading dbldatagen-0.4.0.post1-py3-none-any.whl (122 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.8/122.8 kB 3.6 MB/s eta 0:00:00\nInstalling collected packages: dbldatagen\nSuccessfully installed dbldatagen-0.4.0.post1\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%run \"/Users/cabreirajm@gmail.com/DataPipelineCabreira/Helpers/data_generator\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66d62ab8-f059-4ecd-afa5-bb95afed9b19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.0 Create `Gold Zone` Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dd56966-dd4a-4a81-ac81-18e21a99a82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a35b5d6-0dfc-46a6-a536-493cd285a6c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.0 `Sales Star Schema` Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e59efc0-3761-4546-9e25-fc1431df0e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aiming to optimize queries in large datasets, we can use a dimensional model. \n",
    "We will use Ralph Kimball data warehouse principles and build a Star Schema model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "352565aa-3b9a-435b-8621-413a1c0e091a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### `Dimensional Tables`\n",
    "- **dim_calendar** - Dimension with date information\n",
    "- **dim_cod** - Dimensions with codes  - Low cardinality Dimensions (Junk Dimension): \n",
    "  - **user_origin** - API vs. Files\n",
    "  - **access_from** - mobile vs. computer\n",
    "  - **payment_method** - Pix vs. Boleto vs. Cartão\n",
    "  - **percent_discount** - 5% vs. 10% vs. 15%\n",
    "- **dim_courses** - Dimensão responsável por armazenar as informações de Curso.\n",
    "- **dim_user** - Dimensão responsável por armazenar as informações de Alunos.\n",
    "\n",
    "\n",
    "All tables will have a **Surrogate Key (SK)** column that will be creeated with the **`<col_name> BIGINT GENERATED ALWAYS AS IDENTITY`** command. Spark will populate this column in execution time with an incremental value (incremental(1,1). )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20a9aec8-5dc9-4968-84d6-d3e8e50b8931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.1 `Sale Dimensions`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5dfd8be-7dcf-4a62-b7a3-3ff46861293f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mParseException\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1039232445426085>, line 45\u001B[0m\n",
       "\u001B[1;32m     22\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\u001B[1;32m     23\u001B[0m \u001B[38;5;124m    CREATE TABLE IF NOT EXISTS gold.dim_cod (\u001B[39m\n",
       "\u001B[1;32m     24\u001B[0m \u001B[38;5;124m        sk_cod BIGINT GENERATED ALWAYS AS IDENTITY,\u001B[39m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     30\u001B[0m \u001B[38;5;124m    )  \u001B[39m\n",
       "\u001B[1;32m     31\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m)\n",
       "\u001B[1;32m     33\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\u001B[1;32m     34\u001B[0m \u001B[38;5;124m    CREATE TABLE IF NOT EXISTS gold.dim_course(\u001B[39m\n",
       "\u001B[1;32m     35\u001B[0m \u001B[38;5;124m    sk_course BIGINT GENERATED ALWAYS AS IDENTITY,\u001B[39m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     42\u001B[0m \u001B[38;5;124m    )          \u001B[39m\n",
       "\u001B[1;32m     43\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m)\n",
       "\u001B[0;32m---> 45\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;124m    CREATE TABLE IF NOT EXISTS gold.dim_user(\u001B[39m\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;124m    sk_user BIGINT GENERATED ALWAYS AS IDENTITY,\u001B[39m\n",
       "\u001B[1;32m     48\u001B[0m \u001B[38;5;124m    user_uuid STRING,\u001B[39m\n",
       "\u001B[1;32m     49\u001B[0m \u001B[38;5;124m    name_user STRING,\u001B[39m\n",
       "\u001B[1;32m     50\u001B[0m \u001B[38;5;124m    user_email STRING,\u001B[39m\n",
       "\u001B[1;32m     51\u001B[0m \u001B[38;5;124m    user_age INT, \u001B[39m\n",
       "\u001B[1;32m     52\u001B[0m \u001B[38;5;124m    user_gender STRING,\u001B[39m\n",
       "\u001B[1;32m     53\u001B[0m \u001B[38;5;124m    user_state STRING,\u001B[39m\n",
       "\u001B[1;32m     54\u001B[0m \u001B[38;5;124m    user_profession STRING,\u001B[39m\n",
       "\u001B[1;32m     55\u001B[0m \u001B[38;5;124m    company STRING,\u001B[39m\n",
       "\u001B[1;32m     56\u001B[0m \u001B[38;5;124m    dt_load\u001B[39m\n",
       "\u001B[1;32m     57\u001B[0m \u001B[38;5;124m    )\u001B[39m\n",
       "\u001B[1;32m     58\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1602\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1598\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1599\u001B[0m         litArgs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoArray(\n",
       "\u001B[1;32m   1600\u001B[0m             [_to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m [])]\n",
       "\u001B[1;32m   1601\u001B[0m         )\n",
       "\u001B[0;32m-> 1602\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1603\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1604\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mParseException\u001B[0m: \n",
       "[PARSE_SYNTAX_ERROR] Syntax error at or near ')'.(line 13, pos 4)\n",
       "\n",
       "== SQL ==\n",
       "\n",
       "    CREATE TABLE IF NOT EXISTS gold.dim_user(\n",
       "    sk_user BIGINT GENERATED ALWAYS AS IDENTITY,\n",
       "    user_uuid STRING,\n",
       "    name_user STRING,\n",
       "    user_email STRING,\n",
       "    user_age INT, \n",
       "    user_gender STRING,\n",
       "    user_state STRING,\n",
       "    user_profession STRING,\n",
       "    company STRING,\n",
       "    dt_load\n",
       "    )\n",
       "----^^^\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mParseException\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-1039232445426085>, line 45\u001B[0m\n\u001B[1;32m     22\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;124m    CREATE TABLE IF NOT EXISTS gold.dim_cod (\u001B[39m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;124m        sk_cod BIGINT GENERATED ALWAYS AS IDENTITY,\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;124m    )  \u001B[39m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m)\n\u001B[1;32m     33\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;124m    CREATE TABLE IF NOT EXISTS gold.dim_course(\u001B[39m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;124m    sk_course BIGINT GENERATED ALWAYS AS IDENTITY,\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;124m    )          \u001B[39m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m)\n\u001B[0;32m---> 45\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;124m    CREATE TABLE IF NOT EXISTS gold.dim_user(\u001B[39m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;124m    sk_user BIGINT GENERATED ALWAYS AS IDENTITY,\u001B[39m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;124m    user_uuid STRING,\u001B[39m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124m    name_user STRING,\u001B[39m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;124m    user_email STRING,\u001B[39m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;124m    user_age INT, \u001B[39m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;124m    user_gender STRING,\u001B[39m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;124m    user_state STRING,\u001B[39m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;124m    user_profession STRING,\u001B[39m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;124m    company STRING,\u001B[39m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;124m    dt_load\u001B[39m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;124m    )\u001B[39m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1602\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1598\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1599\u001B[0m         litArgs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoArray(\n\u001B[1;32m   1600\u001B[0m             [_to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m [])]\n\u001B[1;32m   1601\u001B[0m         )\n\u001B[0;32m-> 1602\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1603\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1604\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mParseException\u001B[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near ')'.(line 13, pos 4)\n\n== SQL ==\n\n    CREATE TABLE IF NOT EXISTS gold.dim_user(\n    sk_user BIGINT GENERATED ALWAYS AS IDENTITY,\n    user_uuid STRING,\n    name_user STRING,\n    user_email STRING,\n    user_age INT, \n    user_gender STRING,\n    user_state STRING,\n    user_profession STRING,\n    company STRING,\n    dt_load\n    )\n----^^^\n",
       "errorSummary": "<span class='ansi-red-fg'>ParseException</span>: \n[PARSE_SYNTAX_ERROR] Syntax error at or near ')'.(line 13, pos 4)\n\n== SQL ==\n\n    CREATE TABLE IF NOT EXISTS gold.dim_user(\n    sk_user BIGINT GENERATED ALWAYS AS IDENTITY,\n    user_uuid STRING,\n    name_user STRING,\n    user_email STRING,\n    user_age INT, \n    user_gender STRING,\n    user_state STRING,\n    user_profession STRING,\n    company STRING,\n    dt_load\n    )\n----^^^\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql( \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.dim_calendar(\n",
    "        sk_tempo BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "        date DATE,\n",
    "        year INT, \n",
    "        month STRING,\n",
    "        month_year INT,\n",
    "        day_week_int INT, \n",
    "        day_week STRING,\n",
    "        fl_day_week BOOLEAN,\n",
    "        day_month INT,\n",
    "        fl_last_month_day INT,\n",
    "        day_year INT,\n",
    "        week_year INT,\n",
    "        bimonthly INT,\n",
    "        quarter INT, \n",
    "        semester INT, \n",
    "        dt_load TIMESTAMP\n",
    "    )  \n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.dim_cod (\n",
    "        sk_cod BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "        user_origin STRING,\n",
    "        access_from STRING,\n",
    "        payment_method STRING,\n",
    "        percent_discount STRING,\n",
    "        dt_load TIMESTAMP\n",
    "    )  \n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.dim_course(\n",
    "    sk_course BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    course_uuid STRING,\n",
    "    course_name STRING, \n",
    "    course_level STRING,\n",
    "    cource_price DECIMAL(9,2),\n",
    "    dt_carga TIMESTAMP\n",
    "\n",
    "    )          \n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.dim_user(\n",
    "    sk_user BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    user_uuid STRING,\n",
    "    name_user STRING,\n",
    "    user_email STRING,\n",
    "    user_age INT, \n",
    "    user_gender STRING,\n",
    "    user_state STRING,\n",
    "    user_profession STRING,\n",
    "    company STRING,\n",
    "    dt_load\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a15c2fd-cc5d-4570-ab53-41e552ef64cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.2 `Calendar Dimension`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "203a1c20-ea0b-4d19-91aa-02052f620db0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The view **`vw_dim_calendar`**:\n",
    "* Starts date : **01/06/2024** \n",
    "* End date: **31/12/2025**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f025b7ab-3dc9-442b-b865-ba83670b7aa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>date</th><th>year</th><th>month</th><th>month_year</th><th>day_week_int</th><th>day_week</th><th>fl_day_week</th><th>day_month</th><th>fl_last_month_day</th><th>day_year</th><th>week_year</th><th>bimonthly</th><th>quarter</th><th>semester</th></tr></thead><tbody><tr><td>2024-06-01</td><td>2024</td><td>June</td><td>6</td><td>7</td><td>Saturday</td><td>false</td><td>1</td><td>false</td><td>153</td><td>22</td><td>3</td><td>2</td><td>1</td></tr><tr><td>2024-06-02</td><td>2024</td><td>June</td><td>6</td><td>1</td><td>Sunday</td><td>false</td><td>2</td><td>false</td><td>154</td><td>22</td><td>3</td><td>2</td><td>1</td></tr><tr><td>2024-06-03</td><td>2024</td><td>June</td><td>6</td><td>2</td><td>Monday</td><td>true</td><td>3</td><td>false</td><td>155</td><td>23</td><td>3</td><td>2</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-06-01",
         2024,
         "June",
         6,
         7,
         "Saturday",
         false,
         1,
         false,
         153,
         22,
         3,
         2,
         1
        ],
        [
         "2024-06-02",
         2024,
         "June",
         6,
         1,
         "Sunday",
         false,
         2,
         false,
         154,
         22,
         3,
         2,
         1
        ],
        [
         "2024-06-03",
         2024,
         "June",
         6,
         2,
         "Monday",
         true,
         3,
         false,
         155,
         23,
         3,
         2,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "month_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "day_week_int",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "day_week",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "fl_day_week",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "day_month",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "fl_last_month_day",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "day_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "week_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "bimonthly",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "quarter",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "semester",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import explode, sequence, to_date\n",
    "\n",
    "start_date = '2024-06-01'\n",
    "end_date = '2025-12-31'\n",
    "\n",
    "spark.sql(f\"\"\"         \n",
    "  with dates as (\n",
    "    select\n",
    "      explode(\n",
    "        sequence(\n",
    "          to_date('{start_date}'),\n",
    "          to_date('{end_date}'),\n",
    "          interval 1 day\n",
    "        )\n",
    "      ) as date\n",
    "  )\n",
    "  select\n",
    "    date,\n",
    "    year(date) AS year,\n",
    "    to_csv(\n",
    "      named_struct('date', date),\n",
    "      map('dateFormat', 'MMMM', 'locale', 'EN')\n",
    "    ) AS month,\n",
    "    month(date) as month_year,\n",
    "    dayofweek(date) AS day_week_int,\n",
    "    to_csv(\n",
    "      named_struct('date', date),\n",
    "      map('dateFormat', 'EEEE', 'locale', 'EN')\n",
    "    ) AS day_week,\n",
    "    case\n",
    "      when weekday(date) < 5 then True\n",
    "      else False\n",
    "    end as fl_day_week,\n",
    "    dayofmonth(date) as day_month,\n",
    "    case\n",
    "      when date = last_day(date) then True\n",
    "      else False\n",
    "    end as fl_last_month_day,\n",
    "    dayofyear(date) as day_year,\n",
    "    weekofyear(date) as week_year,\n",
    "    case\n",
    "      when month(date) in (1, 2) then 1\n",
    "      when month(date) in (3, 4) then 2\n",
    "      when month(date) in (5, 6) then 3\n",
    "      when month(date) in (7, 8) then 4\n",
    "      when month(date) in (9, 10) then 5\n",
    "      when month(date) in (11, 12) then 6\n",
    "    end as bimonthly,\n",
    "    case\n",
    "      when month(date) in (1, 2, 3) then 1\n",
    "      when month(date) in (4, 5, 6) then 2\n",
    "      when month(date) in (7, 8, 9) then 3\n",
    "      when month(date) in (10, 11, 12) then 4\n",
    "    end as quarter,\n",
    "    case\n",
    "      when month(date) in (1, 2, 3, 4, 5, 6) then 1\n",
    "      when month(date) in (7, 8, 9, 10, 11, 12) then 2\n",
    "    end as semester\n",
    "  from\n",
    "    dates\n",
    "\"\"\").createOrReplaceTempView('vw_dim_calendar')\n",
    "\n",
    "spark.sql('SELECT * FROM vw_dim_calendar LIMIT 3').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e544026d-8db6-49d2-94e1-05df91001d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "We will now use the **`Merge`** commando to load the `dim_calendar` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00abe91-9f51-4571-8648-edabb7f20d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_updated_rows</th><th>num_deleted_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>579</td><td>0</td><td>0</td><td>579</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         579,
         0,
         0,
         579
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_updated_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_deleted_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sk_tempo</th><th>date</th><th>year</th><th>month</th><th>month_year</th><th>day_week_int</th><th>day_week</th><th>fl_day_week</th><th>day_month</th><th>fl_last_month_day</th><th>day_year</th><th>week_year</th><th>bimonthly</th><th>quarter</th><th>semester</th><th>dt_load</th></tr></thead><tbody><tr><td>1</td><td>2024-06-01</td><td>2024</td><td>June</td><td>6</td><td>7</td><td>Saturday</td><td>false</td><td>1</td><td>0</td><td>153</td><td>22</td><td>3</td><td>2</td><td>1</td><td>2024-11-25T10:27:01.396Z</td></tr><tr><td>2</td><td>2024-06-02</td><td>2024</td><td>June</td><td>6</td><td>1</td><td>Sunday</td><td>false</td><td>2</td><td>0</td><td>154</td><td>22</td><td>3</td><td>2</td><td>1</td><td>2024-11-25T10:27:01.396Z</td></tr><tr><td>3</td><td>2024-06-03</td><td>2024</td><td>June</td><td>6</td><td>2</td><td>Monday</td><td>true</td><td>3</td><td>0</td><td>155</td><td>23</td><td>3</td><td>2</td><td>1</td><td>2024-11-25T10:27:01.396Z</td></tr><tr><td>4</td><td>2024-06-04</td><td>2024</td><td>June</td><td>6</td><td>3</td><td>Tuesday</td><td>true</td><td>4</td><td>0</td><td>156</td><td>23</td><td>3</td><td>2</td><td>1</td><td>2024-11-25T10:27:01.396Z</td></tr><tr><td>5</td><td>2024-06-05</td><td>2024</td><td>June</td><td>6</td><td>4</td><td>Wednesday</td><td>true</td><td>5</td><td>0</td><td>157</td><td>23</td><td>3</td><td>2</td><td>1</td><td>2024-11-25T10:27:01.396Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "2024-06-01",
         2024,
         "June",
         6,
         7,
         "Saturday",
         false,
         1,
         0,
         153,
         22,
         3,
         2,
         1,
         "2024-11-25T10:27:01.396Z"
        ],
        [
         2,
         "2024-06-02",
         2024,
         "June",
         6,
         1,
         "Sunday",
         false,
         2,
         0,
         154,
         22,
         3,
         2,
         1,
         "2024-11-25T10:27:01.396Z"
        ],
        [
         3,
         "2024-06-03",
         2024,
         "June",
         6,
         2,
         "Monday",
         true,
         3,
         0,
         155,
         23,
         3,
         2,
         1,
         "2024-11-25T10:27:01.396Z"
        ],
        [
         4,
         "2024-06-04",
         2024,
         "June",
         6,
         3,
         "Tuesday",
         true,
         4,
         0,
         156,
         23,
         3,
         2,
         1,
         "2024-11-25T10:27:01.396Z"
        ],
        [
         5,
         "2024-06-05",
         2024,
         "June",
         6,
         4,
         "Wednesday",
         true,
         5,
         0,
         157,
         23,
         3,
         2,
         1,
         "2024-11-25T10:27:01.396Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sk_tempo",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "month_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "day_week_int",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "day_week",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "fl_day_week",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "day_month",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "fl_last_month_day",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "day_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "week_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "bimonthly",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "quarter",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "semester",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dt_load",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    MERGE INTO gold.dim_calendar as dest\n",
    "    USING vw_dim_calendar AS orig\n",
    "        ON dest.date = orig.date\n",
    "    WHEN NOT MATCHED    \n",
    "        THEN INSERT(\n",
    "            date,year,month,month_year,day_week_int,day_week,fl_day_week,day_month,fl_last_month_day,day_year,week_year,bimonthly,quarter,semester,dt_load\n",
    "            )\n",
    "        VALUES(\n",
    "            date,year,month,month_year,day_week_int,day_week,fl_day_week,day_month,fl_last_month_day,day_year,week_year,bimonthly,quarter,semester,getdate()\n",
    "        )\n",
    "    \"\"\").display()\n",
    "\n",
    "\n",
    "spark.sql(\"SELECT * FROM gold.dim_calendar LIMIT 5\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2b85c2a-0f7b-44ec-8436-f7790267e4da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Load_Gold_Zone",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
