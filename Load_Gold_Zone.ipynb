{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81574298-f441-4f41-aee8-7275eec42cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# # Loading Data To Gold Zone \n",
    "\n",
    "**This Notebook:**\n",
    "* Load data to Golg Zone of the Data Lake House\n",
    "* Star Schekma and One Big Table Modeling\n",
    "* Creates **`IDENTITY`** column in Databricks delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecd3c5b5-8d45-4392-a9f5-73a260112d4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.0 Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "97e69ae5-8acb-447a-ac48-de146404d808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting dbldatagen\n  Using cached dbldatagen-0.4.0.post1-py3-none-any.whl (122 kB)\nInstalling collected packages: dbldatagen\nSuccessfully installed dbldatagen-0.4.0.post1\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%run \"/Users/cabreirajm@gmail.com/DataPipelineCabreira/Helpers/data_generator\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66d62ab8-f059-4ecd-afa5-bb95afed9b19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.0 Create `Gold Zone` Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dd56966-dd4a-4a81-ac81-18e21a99a82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a35b5d6-0dfc-46a6-a536-493cd285a6c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.0 `Sales Star Schema` Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e59efc0-3761-4546-9e25-fc1431df0e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aiming to optimize queries in large datasets, we can use a dimensional model. \n",
    "We will use Ralph Kimball data warehouse principles and build a Star Schema model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "352565aa-3b9a-435b-8621-413a1c0e091a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### `Dimensional Tables`\n",
    "- **dim_calendar** - Dimension with date information\n",
    "- **dim_cod** - Dimensions with codes  - Low cardinality Dimensions (Junk Dimension): \n",
    "  - **user_origin** - API vs. Files\n",
    "  - **access_from** - mobile vs. computer\n",
    "  - **payment_method** - Pix vs. Boleto vs. Cartão\n",
    "  - **percent_discount** - 5% vs. 10% vs. 15%\n",
    "- **dim_courses** - Dimensão responsável por armazenar as informações de Curso.\n",
    "- **dim_user** - Dimensão responsável por armazenar as informações de Alunos.\n",
    "\n",
    "\n",
    "All tables will have a **Surrogate Key (SK)** column that will be creeated with the **`<col_name> BIGINT GENERATED ALWAYS AS IDENTITY`** command. Spark will populate this column in execution time with an incremental value (incremental(1,1). )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20a9aec8-5dc9-4968-84d6-d3e8e50b8931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.1 `Sale Dimensions`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "681f68ef-6420-44ce-921a-86bfb2a6ff04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%sql\n",
    "#DROP SCHEMA IF EXISTS gold CASCADE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd6b532-7d2a-4471-87b6-3fe167b7b8a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">res16: Boolean = false\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">res16: Boolean = false\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs rm -r dbfs:/user/hive/warehouse/gold.db/dim_calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c286be83-8515-4993-a138-3b0bcadf7a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">res17: Boolean = false\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">res17: Boolean = false\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%fs rm -r dbfs:/user/hive/warehouse/gold.db/dim_cod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89e543e0-cc2c-41b4-8e12-1c67838133c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">res18: Boolean = false\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">res18: Boolean = false\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs rm -r dbfs:/user/hive/warehouse/gold.db/dim_course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d87b95-15f2-4cc7-b9c4-66b3ba5433c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">res19: Boolean = false\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">res19: Boolean = false\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs rm -r dbfs:/user/hive/warehouse/gold.db/dim_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5dfd8be-7dcf-4a62-b7a3-3ff46861293f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.dim_calendar(\n",
    "        sk_tempo BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "        date DATE,\n",
    "        year INT, \n",
    "        month STRING,\n",
    "        month_year INT,\n",
    "        day_week_int INT, \n",
    "        day_week STRING,\n",
    "        fl_day_week BOOLEAN,\n",
    "        day_month INT,\n",
    "        fl_last_month_day INT,\n",
    "        day_year INT,\n",
    "        week_year INT,\n",
    "        bimonthly INT,\n",
    "        quarter INT, \n",
    "        semester INT, \n",
    "        dt_load TIMESTAMP\n",
    "    )  \n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.dim_cod (\n",
    "        sk_cod BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "        user_origin STRING,\n",
    "        access_from STRING,\n",
    "        payment_method STRING,\n",
    "        percent_discount STRING,\n",
    "        dt_load TIMESTAMP\n",
    "    )  \n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.dim_course(\n",
    "        sk_course BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "        course_uuid STRING,\n",
    "        course_name STRING, \n",
    "        course_level STRING,\n",
    "        course_price DECIMAL(9,2),\n",
    "        dt_load TIMESTAMP\n",
    "    )          \n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.dim_user(\n",
    "    sk_user BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    user_uuid STRING,\n",
    "    user_name STRING,\n",
    "    user_email STRING,\n",
    "    user_idade INT, \n",
    "    user_gender STRING,\n",
    "    user_state STRING,\n",
    "    user_profession STRING,\n",
    "    company STRING,\n",
    "    dt_load TIMESTAMP\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a15c2fd-cc5d-4570-ab53-41e552ef64cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.2 `Calendar Dimension`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "203a1c20-ea0b-4d19-91aa-02052f620db0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The view **`vw_dim_calendar`**:\n",
    "* Starts date : **01/06/2024** \n",
    "* End date: **31/12/2025**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f025b7ab-3dc9-442b-b865-ba83670b7aa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>date</th><th>year</th><th>month</th><th>month_year</th><th>day_week_int</th><th>day_week</th><th>fl_day_week</th><th>day_month</th><th>fl_last_month_day</th><th>day_year</th><th>week_year</th><th>bimonthly</th><th>quarter</th><th>semester</th></tr></thead><tbody><tr><td>2024-06-01</td><td>2024</td><td>June</td><td>6</td><td>7</td><td>Saturday</td><td>false</td><td>1</td><td>false</td><td>153</td><td>22</td><td>3</td><td>2</td><td>1</td></tr><tr><td>2024-06-02</td><td>2024</td><td>June</td><td>6</td><td>1</td><td>Sunday</td><td>false</td><td>2</td><td>false</td><td>154</td><td>22</td><td>3</td><td>2</td><td>1</td></tr><tr><td>2024-06-03</td><td>2024</td><td>June</td><td>6</td><td>2</td><td>Monday</td><td>true</td><td>3</td><td>false</td><td>155</td><td>23</td><td>3</td><td>2</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-06-01",
         2024,
         "June",
         6,
         7,
         "Saturday",
         false,
         1,
         false,
         153,
         22,
         3,
         2,
         1
        ],
        [
         "2024-06-02",
         2024,
         "June",
         6,
         1,
         "Sunday",
         false,
         2,
         false,
         154,
         22,
         3,
         2,
         1
        ],
        [
         "2024-06-03",
         2024,
         "June",
         6,
         2,
         "Monday",
         true,
         3,
         false,
         155,
         23,
         3,
         2,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "month_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "day_week_int",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "day_week",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "fl_day_week",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "day_month",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "fl_last_month_day",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "day_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "week_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "bimonthly",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "quarter",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "semester",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import explode, sequence, to_date\n",
    "\n",
    "start_date = '2024-06-01'\n",
    "end_date = '2025-12-31'\n",
    "\n",
    "spark.sql(f\"\"\"         \n",
    "  with dates as (\n",
    "    select\n",
    "      explode(\n",
    "        sequence(\n",
    "          to_date('{start_date}'),\n",
    "          to_date('{end_date}'),\n",
    "          interval 1 day\n",
    "        )\n",
    "      ) as date\n",
    "  )\n",
    "  select\n",
    "    date,\n",
    "    year(date) AS year,\n",
    "    to_csv(\n",
    "      named_struct('date', date),\n",
    "      map('dateFormat', 'MMMM', 'locale', 'EN')\n",
    "    ) AS month,\n",
    "    month(date) as month_year,\n",
    "    dayofweek(date) AS day_week_int,\n",
    "    to_csv(\n",
    "      named_struct('date', date),\n",
    "      map('dateFormat', 'EEEE', 'locale', 'EN')\n",
    "    ) AS day_week,\n",
    "    case\n",
    "      when weekday(date) < 5 then True\n",
    "      else False\n",
    "    end as fl_day_week,\n",
    "    dayofmonth(date) as day_month,\n",
    "    case\n",
    "      when date = last_day(date) then True\n",
    "      else False\n",
    "    end as fl_last_month_day,\n",
    "    dayofyear(date) as day_year,\n",
    "    weekofyear(date) as week_year,\n",
    "    case\n",
    "      when month(date) in (1, 2) then 1\n",
    "      when month(date) in (3, 4) then 2\n",
    "      when month(date) in (5, 6) then 3\n",
    "      when month(date) in (7, 8) then 4\n",
    "      when month(date) in (9, 10) then 5\n",
    "      when month(date) in (11, 12) then 6\n",
    "    end as bimonthly,\n",
    "    case\n",
    "      when month(date) in (1, 2, 3) then 1\n",
    "      when month(date) in (4, 5, 6) then 2\n",
    "      when month(date) in (7, 8, 9) then 3\n",
    "      when month(date) in (10, 11, 12) then 4\n",
    "    end as quarter,\n",
    "    case\n",
    "      when month(date) in (1, 2, 3, 4, 5, 6) then 1\n",
    "      when month(date) in (7, 8, 9, 10, 11, 12) then 2\n",
    "    end as semester\n",
    "  from\n",
    "    dates\n",
    "\"\"\").createOrReplaceTempView('vw_dim_calendar')\n",
    "\n",
    "spark.sql('SELECT * FROM vw_dim_calendar LIMIT 3').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e544026d-8db6-49d2-94e1-05df91001d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "We will now use the **`Merge`** commando to load the `dim_calendar` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00abe91-9f51-4571-8648-edabb7f20d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_updated_rows</th><th>num_deleted_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>579</td><td>0</td><td>0</td><td>579</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         579,
         0,
         0,
         579
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_updated_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_deleted_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sk_tempo</th><th>date</th><th>year</th><th>month</th><th>month_year</th><th>day_week_int</th><th>day_week</th><th>fl_day_week</th><th>day_month</th><th>fl_last_month_day</th><th>day_year</th><th>week_year</th><th>bimonthly</th><th>quarter</th><th>semester</th><th>dt_load</th></tr></thead><tbody><tr><td>1</td><td>2024-06-01</td><td>2024</td><td>June</td><td>6</td><td>7</td><td>Saturday</td><td>false</td><td>1</td><td>0</td><td>153</td><td>22</td><td>3</td><td>2</td><td>1</td><td>2024-11-28T01:32:25.472Z</td></tr><tr><td>2</td><td>2024-06-02</td><td>2024</td><td>June</td><td>6</td><td>1</td><td>Sunday</td><td>false</td><td>2</td><td>0</td><td>154</td><td>22</td><td>3</td><td>2</td><td>1</td><td>2024-11-28T01:32:25.472Z</td></tr><tr><td>3</td><td>2024-06-03</td><td>2024</td><td>June</td><td>6</td><td>2</td><td>Monday</td><td>true</td><td>3</td><td>0</td><td>155</td><td>23</td><td>3</td><td>2</td><td>1</td><td>2024-11-28T01:32:25.472Z</td></tr><tr><td>4</td><td>2024-06-04</td><td>2024</td><td>June</td><td>6</td><td>3</td><td>Tuesday</td><td>true</td><td>4</td><td>0</td><td>156</td><td>23</td><td>3</td><td>2</td><td>1</td><td>2024-11-28T01:32:25.472Z</td></tr><tr><td>5</td><td>2024-06-05</td><td>2024</td><td>June</td><td>6</td><td>4</td><td>Wednesday</td><td>true</td><td>5</td><td>0</td><td>157</td><td>23</td><td>3</td><td>2</td><td>1</td><td>2024-11-28T01:32:25.472Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "2024-06-01",
         2024,
         "June",
         6,
         7,
         "Saturday",
         false,
         1,
         0,
         153,
         22,
         3,
         2,
         1,
         "2024-11-28T01:32:25.472Z"
        ],
        [
         2,
         "2024-06-02",
         2024,
         "June",
         6,
         1,
         "Sunday",
         false,
         2,
         0,
         154,
         22,
         3,
         2,
         1,
         "2024-11-28T01:32:25.472Z"
        ],
        [
         3,
         "2024-06-03",
         2024,
         "June",
         6,
         2,
         "Monday",
         true,
         3,
         0,
         155,
         23,
         3,
         2,
         1,
         "2024-11-28T01:32:25.472Z"
        ],
        [
         4,
         "2024-06-04",
         2024,
         "June",
         6,
         3,
         "Tuesday",
         true,
         4,
         0,
         156,
         23,
         3,
         2,
         1,
         "2024-11-28T01:32:25.472Z"
        ],
        [
         5,
         "2024-06-05",
         2024,
         "June",
         6,
         4,
         "Wednesday",
         true,
         5,
         0,
         157,
         23,
         3,
         2,
         1,
         "2024-11-28T01:32:25.472Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sk_tempo",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "month_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "day_week_int",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "day_week",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "fl_day_week",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "day_month",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "fl_last_month_day",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "day_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "week_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "bimonthly",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "quarter",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "semester",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dt_load",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    MERGE INTO gold.dim_calendar as dest\n",
    "    USING vw_dim_calendar AS orig\n",
    "        ON dest.date = orig.date\n",
    "    WHEN NOT MATCHED    \n",
    "        THEN INSERT(\n",
    "            date,year,month,month_year,day_week_int,day_week,fl_day_week,day_month,fl_last_month_day,day_year,week_year,bimonthly,quarter,semester,dt_load\n",
    "            )\n",
    "        VALUES(\n",
    "            date,year,month,month_year,day_week_int,day_week,fl_day_week,day_month,fl_last_month_day,day_year,week_year,bimonthly,quarter,semester,getdate()\n",
    "        )\n",
    "    \"\"\").display()\n",
    "\n",
    "\n",
    "spark.sql(\"SELECT * FROM gold.dim_calendar LIMIT 5\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2b85c2a-0f7b-44ec-8436-f7790267e4da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.3 Junk Dimension\n",
    "We will first create a **`vw_dim_cod`** view that will be responsible for creating a **cartesian product between some of low cardinality codes***. \n",
    "\n",
    "We will use this method in order to avoid building low dimensions with few registers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "756495e4-8560-4216-abc6-27bd8849d9b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_origin</th><th>local_access</th><th>payment_method</th><th>percent_discount</th></tr></thead><tbody><tr><td>API</td><td>Computer</td><td>boleto</td><td>5%</td></tr><tr><td>API</td><td>Computer</td><td>boleto</td><td>do not apply</td></tr><tr><td>API</td><td>Computer</td><td>credito</td><td>10%</td></tr><tr><td>API</td><td>Computer</td><td>credito</td><td>15%</td></tr><tr><td>API</td><td>Computer</td><td>credito</td><td>5%</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "API",
         "Computer",
         "boleto",
         "5%"
        ],
        [
         "API",
         "Computer",
         "boleto",
         "do not apply"
        ],
        [
         "API",
         "Computer",
         "credito",
         "10%"
        ],
        [
         "API",
         "Computer",
         "credito",
         "15%"
        ],
        [
         "API",
         "Computer",
         "credito",
         "5%"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "user_origin",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "local_access",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "payment_method",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "percent_discount",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql( \"\"\"\n",
    "    SELECT DISTINCT \n",
    "    s.origin user_origin ,\n",
    "    CASE WHEN s.origin = 'File' then \"do not apply\" ELSE a.local_access end as local_access,\n",
    "    s.payment_method,\n",
    "    coalesce(s.percent_discount, 'do not apply' ) as percent_discount\n",
    "    from silver.tb_sales as s\n",
    "    cross join silver.tb_access as a \n",
    "    order BY user_origin,local_access, payment_method, percent_discount      \n",
    "\"\"\").createOrReplaceTempView(\"vw_dim_cod\")\n",
    "\n",
    "spark.sql(\"select * from vw_dim_cod limit 5\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da1676d-cc6d-472e-90e2-042a4a0cbfbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_updated_rows</th><th>num_deleted_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>22</td><td>0</td><td>0</td><td>22</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         22,
         0,
         0,
         22
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_updated_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_deleted_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sk_cod</th><th>user_origin</th><th>access_from</th><th>payment_method</th><th>percent_discount</th><th>dt_load</th></tr></thead><tbody><tr><td>1</td><td>API</td><td>Computer</td><td>boleto</td><td>5%</td><td>2024-11-28T01:32:35.359Z</td></tr><tr><td>2</td><td>API</td><td>Computer</td><td>boleto</td><td>do not apply</td><td>2024-11-28T01:32:35.359Z</td></tr><tr><td>3</td><td>API</td><td>Computer</td><td>credito</td><td>10%</td><td>2024-11-28T01:32:35.359Z</td></tr><tr><td>4</td><td>API</td><td>Computer</td><td>credito</td><td>15%</td><td>2024-11-28T01:32:35.359Z</td></tr><tr><td>5</td><td>API</td><td>Computer</td><td>credito</td><td>5%</td><td>2024-11-28T01:32:35.359Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "API",
         "Computer",
         "boleto",
         "5%",
         "2024-11-28T01:32:35.359Z"
        ],
        [
         2,
         "API",
         "Computer",
         "boleto",
         "do not apply",
         "2024-11-28T01:32:35.359Z"
        ],
        [
         3,
         "API",
         "Computer",
         "credito",
         "10%",
         "2024-11-28T01:32:35.359Z"
        ],
        [
         4,
         "API",
         "Computer",
         "credito",
         "15%",
         "2024-11-28T01:32:35.359Z"
        ],
        [
         5,
         "API",
         "Computer",
         "credito",
         "5%",
         "2024-11-28T01:32:35.359Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sk_cod",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "user_origin",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "access_from",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "payment_method",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "percent_discount",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dt_load",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    MERGE INTO gold.dim_cod as dest\n",
    "    using vw_dim_cod as orig\n",
    "    on dest.user_origin = orig.user_origin\n",
    "        and dest.payment_method = orig.payment_method\n",
    "        and dest.access_from = orig.local_access\n",
    "        and dest.percent_discount = orig.percent_discount\n",
    "\n",
    "    when not matched \n",
    "        then INSERT (\n",
    "            user_origin,\n",
    "            access_from,\n",
    "            payment_method,\n",
    "            percent_discount,\n",
    "           dt_load\n",
    "        )\n",
    "        values(\n",
    "            user_origin,\n",
    "            local_access,\n",
    "            payment_method,\n",
    "            percent_discount,\n",
    "           getdate()\n",
    "        )       \n",
    "\"\"\").display()\n",
    "\n",
    "spark.sql(\"SELECT * FROM gold.dim_cod LIMIT 5\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75d1554d-64f2-4f65-aeaf-d271b4c36a6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8912832b-5a00-4217-9cb6-2c45a58d6362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.4 User Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb5375b3-4d07-4621-a87d-41d005dbdb2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_updated_rows</th><th>num_deleted_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>157185</td><td>0</td><td>0</td><td>157185</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         157185,
         0,
         0,
         157185
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_updated_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_deleted_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sk_user</th><th>user_uuid</th><th>user_name</th><th>user_email</th><th>user_idade</th><th>user_gender</th><th>user_state</th><th>user_profession</th><th>company</th><th>dt_load</th></tr></thead><tbody><tr><td>1</td><td>8d1a271791b6a3fb5af5c34031eaae4f</td><td>Usuario 5250b1b2f4</td><td>usuario_5250b1b2f4@uol.com</td><td>32</td><td>F</td><td>PE</td><td>Analista de BI</td><td>null</td><td>2024-11-28T01:32:45.471Z</td></tr><tr><td>8</td><td>2be5820316b8dfad2d6d5f447eec1e71</td><td>Usuario 551b70427a</td><td>usuario_551b70427a@uol.com</td><td>27</td><td>M</td><td>PR</td><td>Analista de Dados</td><td>null</td><td>2024-11-28T01:32:45.471Z</td></tr><tr><td>15</td><td>5d7cc1629ba8d330729524fc8e8fb86e</td><td>Usuario 1c84387f7d</td><td>usuario_1c84387f7d@hotmail.com</td><td>32</td><td>F</td><td>AC</td><td>Arquiteto de Dados</td><td>null</td><td>2024-11-28T01:32:45.471Z</td></tr><tr><td>22</td><td>5c44a8122ab24c978b35c79548f72a43</td><td>Usuario 2935de7d69</td><td>usuario_2935de7d69@uol.com</td><td>26</td><td>M</td><td>TO</td><td>Engenheiro de Dados</td><td>null</td><td>2024-11-28T01:32:45.471Z</td></tr><tr><td>29</td><td>a550627644f51e197c94935ad31de86f</td><td>Usuario 528ec8140b</td><td>usuario_528ec8140b@uol.com</td><td>42</td><td>F</td><td>SE</td><td>Cientista de Dados</td><td>null</td><td>2024-11-28T01:32:45.471Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "8d1a271791b6a3fb5af5c34031eaae4f",
         "Usuario 5250b1b2f4",
         "usuario_5250b1b2f4@uol.com",
         32,
         "F",
         "PE",
         "Analista de BI",
         null,
         "2024-11-28T01:32:45.471Z"
        ],
        [
         8,
         "2be5820316b8dfad2d6d5f447eec1e71",
         "Usuario 551b70427a",
         "usuario_551b70427a@uol.com",
         27,
         "M",
         "PR",
         "Analista de Dados",
         null,
         "2024-11-28T01:32:45.471Z"
        ],
        [
         15,
         "5d7cc1629ba8d330729524fc8e8fb86e",
         "Usuario 1c84387f7d",
         "usuario_1c84387f7d@hotmail.com",
         32,
         "F",
         "AC",
         "Arquiteto de Dados",
         null,
         "2024-11-28T01:32:45.471Z"
        ],
        [
         22,
         "5c44a8122ab24c978b35c79548f72a43",
         "Usuario 2935de7d69",
         "usuario_2935de7d69@uol.com",
         26,
         "M",
         "TO",
         "Engenheiro de Dados",
         null,
         "2024-11-28T01:32:45.471Z"
        ],
        [
         29,
         "a550627644f51e197c94935ad31de86f",
         "Usuario 528ec8140b",
         "usuario_528ec8140b@uol.com",
         42,
         "F",
         "SE",
         "Cientista de Dados",
         null,
         "2024-11-28T01:32:45.471Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sk_user",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "user_uuid",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "user_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "user_email",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "user_idade",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "user_gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "user_state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "user_profession",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "company",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dt_load",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "      MERGE INTO gold.dim_user as dest \n",
    "      using silver.tb_users as orig\n",
    "        on dest.user_uuid = orig.user_uuid\n",
    "    when matched\n",
    "        and dest.user_email != orig.user_email\n",
    "        or dest.user_idade!= orig.user_idade\n",
    "        or dest.user_state != orig.user_state\n",
    "        or dest.user_profession != orig.user_profession\n",
    "        or dest.company != orig.company\n",
    "        THEN UPDATE \n",
    "            SET dest.user_email = orig.user_email\n",
    "                ,dest.user_idade = orig.user_idade \n",
    "                ,dest.user_state = orig.user_state\n",
    "                ,dest.user_profession = orig.user_profession\n",
    "                ,dest.company  = orig.company\n",
    "          \n",
    "         when not MATCHED\n",
    "            then insert (\n",
    "        user_uuid, \n",
    "        user_name, \n",
    "        user_email, \n",
    "        user_idade, \n",
    "        user_gender,\n",
    "        user_state, \n",
    "        user_profession, \n",
    "        company, \n",
    "        dt_load\n",
    "            )\n",
    "    VALUES (\n",
    "        user_uuid, \n",
    "        user_name, \n",
    "        user_email, \n",
    "        user_idade, \n",
    "        user_gender,\n",
    "        user_state, \n",
    "        user_profession, \n",
    "        company, \n",
    "        getdate()\n",
    "\n",
    "            )   \n",
    "\"\"\").display()\n",
    "\n",
    "spark.sql(\"SELECT * FROM gold.dim_user LIMIT 5\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4b1c139-f940-49cd-8cc4-c4b7e6d7fe3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.5 Courses Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc6505de-e578-4fce-acd7-3dccc364f06a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We will load the **`dim_course`** with  **`MERGE`**  command\n",
    "* Source: **`silver.tb_curso`**. table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e024b89a-8bb8-4ead-9cb8-a2e9be959075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_updated_rows</th><th>num_deleted_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>3</td><td>0</td><td>0</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         0,
         0,
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_updated_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_deleted_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sk_course</th><th>course_uuid</th><th>course_name</th><th>course_level</th><th>course_price</th><th>dt_load</th></tr></thead><tbody><tr><td>1</td><td>fb95df132ca7f41d392bc98ccf0cfeb8</td><td>Data Pipeline with Databricks</td><td>beginner</td><td>589.90</td><td>2024-11-28T01:32:54.016Z</td></tr><tr><td>2</td><td>bda125b01c9596e123e5f9b3bf00f3a8</td><td>From your first data pipeline to a Data Lakehouse with Databricks</td><td>intermediate</td><td>659.90</td><td>2024-11-28T01:32:54.016Z</td></tr><tr><td>3</td><td>ff17869bc6f9d9865e0bf8133c4ce3c3</td><td>Building a Data Pipeline with Spark Structured Streaming</td><td>advanced</td><td>549.90</td><td>2024-11-28T01:32:54.016Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "fb95df132ca7f41d392bc98ccf0cfeb8",
         "Data Pipeline with Databricks",
         "beginner",
         "589.90",
         "2024-11-28T01:32:54.016Z"
        ],
        [
         2,
         "bda125b01c9596e123e5f9b3bf00f3a8",
         "From your first data pipeline to a Data Lakehouse with Databricks",
         "intermediate",
         "659.90",
         "2024-11-28T01:32:54.016Z"
        ],
        [
         3,
         "ff17869bc6f9d9865e0bf8133c4ce3c3",
         "Building a Data Pipeline with Spark Structured Streaming",
         "advanced",
         "549.90",
         "2024-11-28T01:32:54.016Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sk_course",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "course_uuid",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "course_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "course_level",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "course_price",
         "type": "\"decimal(9,2)\""
        },
        {
         "metadata": "{}",
         "name": "dt_load",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    merge into gold.dim_course as dest \n",
    "        using silver.tb_courses as orig\n",
    "            on dest.course_uuid = orig.course_uuid\n",
    "\n",
    "    when matched    \n",
    "        and dest.course_name != orig.course_name\n",
    "        or dest.course_level != orig.course_level\n",
    "        or dest.course_price != orig.course_price\n",
    "        then update \n",
    "            set \n",
    "                dest.course_name = orig.course_name,\n",
    "                dest.course_level = orig.course_level,\n",
    "                dest.course_price = orig.course_price\n",
    "    when not matched\n",
    "        then insert (\n",
    "            course_uuid,\n",
    "            course_name,\n",
    "            course_level,\n",
    "            course_price,\n",
    "            dt_load\n",
    "        )\n",
    "    values (\n",
    "            course_uuid,\n",
    "            course_name,\n",
    "            course_level,\n",
    "            course_price,\n",
    "            GETDATE()\n",
    "    )\n",
    "\"\"\").display()\n",
    "\n",
    "spark.sql(\"SELECT * FROM gold.dim_course LIMIT 5\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a972db9-041b-4793-b3ca-21a5684bf77a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.6 Sales Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab50cdd4-aa34-4b22-a1ff-cb8e4bde3eb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_stream_sales= spark.readStream.table('silver.tb_sales')\n",
    "df_stream_sales.createOrReplaceTempView('vw_fact_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb68924e-ade5-4c75-9f03-73a18f0cae53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3337697916647392>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m df_fact_sales \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;124m    with orig_fact_sales (\u001B[39m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;124m        select \u001B[39m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;124m            s.percent_discount,\u001B[39m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;124m            case when s.origin = \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFILE\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m then \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnot applicable\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m else a.local_access end as access_local,\u001B[39m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;124m            s.payment_method,\u001B[39m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;124m            s.origin as user_origin,\u001B[39m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;124m            s.discount_value,\u001B[39m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;124m            s.total_value,\u001B[39m\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;124m            s.course_uuid,\u001B[39m\n",
       "\u001B[1;32m     11\u001B[0m \u001B[38;5;124m            s.dt_sale\u001B[39m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;124m        from vw_fact_sales as s\u001B[39m\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;124m        left join silver.tb_access as a on s.access_uuid = a.access_uuid\u001B[39m\n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;124m    )    \u001B[39m\n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;124m    select   \u001B[39m\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;124m        dcal.sk_tempo,\u001B[39m\n",
       "\u001B[1;32m     17\u001B[0m \u001B[38;5;124m        dcod.sk_cod,\u001B[39m\n",
       "\u001B[1;32m     18\u001B[0m \u001B[38;5;124m        dc.sk_course,\u001B[39m\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;124m        du.sk_user,\u001B[39m\n",
       "\u001B[1;32m     20\u001B[0m \u001B[38;5;124m        ofs.discount_value,\u001B[39m\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;124m        ofs.total_value\u001B[39m\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;124m    from orig_fact_sales as ofs\u001B[39m\n",
       "\u001B[1;32m     23\u001B[0m \u001B[38;5;124m    join gold.dim_cod as dcod on ofs.payment_method = dcod.payment_method\u001B[39m\n",
       "\u001B[1;32m     24\u001B[0m \u001B[38;5;124m                                and ofs.user_origin = dcod.user_origin\u001B[39m\n",
       "\u001B[1;32m     25\u001B[0m \u001B[38;5;124m                                and ofs.access_local =dcod.access_from\u001B[39m\n",
       "\u001B[1;32m     26\u001B[0m \u001B[38;5;124m                                and ofs.percent_discount = dcod.percent_discount\u001B[39m\n",
       "\u001B[1;32m     27\u001B[0m \u001B[38;5;124m    join gold.dim_calendar as dcal on dcal.date = cast(ofs.dt_sale as date)\u001B[39m\n",
       "\u001B[1;32m     28\u001B[0m \u001B[38;5;124m    join gold.dim_course as dc on dc.course_uuid = ofs.course_uuid\u001B[39m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;124m    join gold.dim_user as du on du.user_uuid = ofs.user_uuid\u001B[39m\n",
       "\u001B[1;32m     30\u001B[0m \n",
       "\u001B[1;32m     31\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m)\n",
       "\u001B[1;32m     33\u001B[0m df_fact_sales\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m5\u001B[39m)\u001B[38;5;241m.\u001B[39mdisplay()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1602\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1598\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1599\u001B[0m         litArgs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoArray(\n",
       "\u001B[1;32m   1600\u001B[0m             [_to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m [])]\n",
       "\u001B[1;32m   1601\u001B[0m         )\n",
       "\u001B[0;32m-> 1602\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1603\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1604\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `ofs`.`user_uuid` cannot be resolved. Did you mean one of the following? [`du`.`user_uuid`, `ofs`.`course_uuid`, `ofs`.`user_origin`, `dc`.`course_uuid`, `du`.`user_email`].; line 29 pos 47;\n",
       "'WithCTE\n",
       ":- ~CTERelationDef 7, false\n",
       ":  +- ~SubqueryAlias orig_fact_sales\n",
       ":     +- ~Project [percent_discount#68131, CASE WHEN (origin#68133 = FILE) THEN not applicable ELSE local_access#69408 END AS access_local#69381, payment_method#68127, origin#68133 AS user_origin#69382, discount_value#68132, total_value#68130, course_uuid#68126, dt_sale#68123]\n",
       ":        +- ~Join LeftOuter, (access_uuid#68124 = access_uuid#69409)\n",
       ":           :- ~SubqueryAlias s\n",
       ":           :  +- ~SubqueryAlias vw_fact_sales\n",
       ":           :     +- View (`vw_fact_sales`, [dt_sale#68123,access_uuid#68124,user_uuid#68125,course_uuid#68126,payment_method#68127,qnt_instalments#68128,instalments_values#68129,total_value#68130,percent_discount#68131,discount_value#68132,origin#68133,dt_load#68134])\n",
       ":           :        +- ~SubqueryAlias spark_catalog.silver.tb_sales\n",
       ":           :           +- ~StreamingRelation DataSource(org.apache.spark.sql.SparkSession@38ab048a,delta,List(),None,List(),None,Map(path -> *********(redacted)),Some(CatalogTable(\n",
       "Catalog: spark_catalog\n",
       "Database: silver\n",
       "Table: tb_sales\n",
       "Owner: root\n",
       "Created Time: Thu Nov 28 00:54:05 UTC 2024\n",
       "Last Access: UNKNOWN\n",
       "Created By: Spark 3.4.1\n",
       "Type: MANAGED\n",
       "Provider: delta\n",
       "Table Properties: [delta.lastCommitTimestamp=1732755244000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\n",
       "Location: dbfs:/user/hive/warehouse/silver.db/tb_sales\n",
       "Serde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n",
       "InputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\n",
       "OutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n",
       "Partition Provider: Catalog\n",
       "Schema: root\n",
       " |-- dt_sale: timestamp (nullable = true)\n",
       " |-- access_uuid: string (nullable = true)\n",
       " |-- user_uuid: string (nullable = true)\n",
       " |-- course_uuid: string (nullable = true)\n",
       " |-- payment_method: string (nullable = true)\n",
       " |-- qnt_instalments: integer (nullable = true)\n",
       " |-- instalments_values: decimal(9,2) (nullable = true)\n",
       " |-- total_value: decimal(9,2) (nullable = true)\n",
       " |-- percent_discount: string (nullable = true)\n",
       " |-- discount_value: decimal(9,2) (nullable = true)\n",
       " |-- origin: string (nullable = true)\n",
       " |-- dt_load: timestamp (nullable = true)\n",
       "))), tahoe, [dt_sale#68123, access_uuid#68124, user_uuid#68125, course_uuid#68126, payment_method#68127, qnt_instalments#68128, instalments_values#68129, total_value#68130, percent_discount#68131, discount_value#68132, origin#68133, dt_load#68134]\n",
       ":           +- SubqueryAlias a\n",
       ":              +- SubqueryAlias spark_catalog.silver.tb_access\n",
       ":                 +- Relation spark_catalog.silver.tb_access[access_timestamp#69406,access_ip_address#69407,local_access#69408,access_uuid#69409,user_uuid#69410,course_uuid#69411,dt_load#69412] parquet\n",
       "+- 'Project ['dcal.sk_tempo, 'dcod.sk_cod, 'dc.sk_course, 'du.sk_user, 'ofs.discount_value, 'ofs.total_value]\n",
       "   +- 'Join Inner, (user_uuid#69464 = 'ofs.user_uuid)\n",
       "      :- Join Inner, (course_uuid#69458 = course_uuid#68126)\n",
       "      :  :- Join Inner, (date#69442 = cast(dt_sale#68123 as date))\n",
       "      :  :  :- Join Inner, (((payment_method#68127 = payment_method#69438) AND (user_origin#69382 = user_origin#69436)) AND ((access_local#69381 = access_from#69437) AND (percent_discount#68131 = percent_discount#69439)))\n",
       "      :  :  :  :- SubqueryAlias ofs\n",
       "      :  :  :  :  +- SubqueryAlias orig_fact_sales\n",
       "      :  :  :  :     +- CTERelationRef 7, true, [percent_discount#68131, access_local#69381, payment_method#68127, user_origin#69382, discount_value#68132, total_value#68130, course_uuid#68126, dt_sale#68123]\n",
       "      :  :  :  +- SubqueryAlias dcod\n",
       "      :  :  :     +- SubqueryAlias spark_catalog.gold.dim_cod\n",
       "      :  :  :        +- Relation spark_catalog.gold.dim_cod[sk_cod#69435L,user_origin#69436,access_from#69437,payment_method#69438,percent_discount#69439,dt_load#69440] parquet\n",
       "      :  :  +- SubqueryAlias dcal\n",
       "      :  :     +- SubqueryAlias spark_catalog.gold.dim_calendar\n",
       "      :  :        +- Relation spark_catalog.gold.dim_calendar[sk_tempo#69441L,date#69442,year#69443,month#69444,month_year#69445,day_week_int#69446,day_week#69447,fl_day_week#69448,day_month#69449,fl_last_month_day#69450,day_year#69451,week_year#69452,bimonthly#69453,quarter#69454,semester#69455,dt_load#69456] parquet\n",
       "      :  +- SubqueryAlias dc\n",
       "      :     +- SubqueryAlias spark_catalog.gold.dim_course\n",
       "      :        +- Relation spark_catalog.gold.dim_course[sk_course#69457L,course_uuid#69458,course_name#69459,course_level#69460,course_price#69461,dt_load#69462] parquet\n",
       "      +- SubqueryAlias du\n",
       "         +- SubqueryAlias spark_catalog.gold.dim_user\n",
       "            +- Relation spark_catalog.gold.dim_user[sk_user#69463L,user_uuid#69464,user_name#69465,user_email#69466,user_idade#69467,user_gender#69468,user_state#69469,user_profession#69470,company#69471,dt_load#69472] parquet\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3337697916647392>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_fact_sales \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124m    with orig_fact_sales (\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124m        select \u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124m            s.percent_discount,\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124m            case when s.origin = \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFILE\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m then \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnot applicable\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m else a.local_access end as access_local,\u001B[39m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124m            s.payment_method,\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124m            s.origin as user_origin,\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124m            s.discount_value,\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124m            s.total_value,\u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124m            s.course_uuid,\u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124m            s.dt_sale\u001B[39m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124m        from vw_fact_sales as s\u001B[39m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;124m        left join silver.tb_access as a on s.access_uuid = a.access_uuid\u001B[39m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;124m    )    \u001B[39m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124m    select   \u001B[39m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124m        dcal.sk_tempo,\u001B[39m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124m        dcod.sk_cod,\u001B[39m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124m        dc.sk_course,\u001B[39m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;124m        du.sk_user,\u001B[39m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;124m        ofs.discount_value,\u001B[39m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;124m        ofs.total_value\u001B[39m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;124m    from orig_fact_sales as ofs\u001B[39m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;124m    join gold.dim_cod as dcod on ofs.payment_method = dcod.payment_method\u001B[39m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;124m                                and ofs.user_origin = dcod.user_origin\u001B[39m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;124m                                and ofs.access_local =dcod.access_from\u001B[39m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;124m                                and ofs.percent_discount = dcod.percent_discount\u001B[39m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;124m    join gold.dim_calendar as dcal on dcal.date = cast(ofs.dt_sale as date)\u001B[39m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;124m    join gold.dim_course as dc on dc.course_uuid = ofs.course_uuid\u001B[39m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;124m    join gold.dim_user as du on du.user_uuid = ofs.user_uuid\u001B[39m\n\u001B[1;32m     30\u001B[0m \n\u001B[1;32m     31\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m)\n\u001B[1;32m     33\u001B[0m df_fact_sales\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m5\u001B[39m)\u001B[38;5;241m.\u001B[39mdisplay()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1602\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1598\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1599\u001B[0m         litArgs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoArray(\n\u001B[1;32m   1600\u001B[0m             [_to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m [])]\n\u001B[1;32m   1601\u001B[0m         )\n\u001B[0;32m-> 1602\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1603\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1604\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `ofs`.`user_uuid` cannot be resolved. Did you mean one of the following? [`du`.`user_uuid`, `ofs`.`course_uuid`, `ofs`.`user_origin`, `dc`.`course_uuid`, `du`.`user_email`].; line 29 pos 47;\n'WithCTE\n:- ~CTERelationDef 7, false\n:  +- ~SubqueryAlias orig_fact_sales\n:     +- ~Project [percent_discount#68131, CASE WHEN (origin#68133 = FILE) THEN not applicable ELSE local_access#69408 END AS access_local#69381, payment_method#68127, origin#68133 AS user_origin#69382, discount_value#68132, total_value#68130, course_uuid#68126, dt_sale#68123]\n:        +- ~Join LeftOuter, (access_uuid#68124 = access_uuid#69409)\n:           :- ~SubqueryAlias s\n:           :  +- ~SubqueryAlias vw_fact_sales\n:           :     +- View (`vw_fact_sales`, [dt_sale#68123,access_uuid#68124,user_uuid#68125,course_uuid#68126,payment_method#68127,qnt_instalments#68128,instalments_values#68129,total_value#68130,percent_discount#68131,discount_value#68132,origin#68133,dt_load#68134])\n:           :        +- ~SubqueryAlias spark_catalog.silver.tb_sales\n:           :           +- ~StreamingRelation DataSource(org.apache.spark.sql.SparkSession@38ab048a,delta,List(),None,List(),None,Map(path -> *********(redacted)),Some(CatalogTable(\nCatalog: spark_catalog\nDatabase: silver\nTable: tb_sales\nOwner: root\nCreated Time: Thu Nov 28 00:54:05 UTC 2024\nLast Access: UNKNOWN\nCreated By: Spark 3.4.1\nType: MANAGED\nProvider: delta\nTable Properties: [delta.lastCommitTimestamp=1732755244000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\nLocation: dbfs:/user/hive/warehouse/silver.db/tb_sales\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog\nSchema: root\n |-- dt_sale: timestamp (nullable = true)\n |-- access_uuid: string (nullable = true)\n |-- user_uuid: string (nullable = true)\n |-- course_uuid: string (nullable = true)\n |-- payment_method: string (nullable = true)\n |-- qnt_instalments: integer (nullable = true)\n |-- instalments_values: decimal(9,2) (nullable = true)\n |-- total_value: decimal(9,2) (nullable = true)\n |-- percent_discount: string (nullable = true)\n |-- discount_value: decimal(9,2) (nullable = true)\n |-- origin: string (nullable = true)\n |-- dt_load: timestamp (nullable = true)\n))), tahoe, [dt_sale#68123, access_uuid#68124, user_uuid#68125, course_uuid#68126, payment_method#68127, qnt_instalments#68128, instalments_values#68129, total_value#68130, percent_discount#68131, discount_value#68132, origin#68133, dt_load#68134]\n:           +- SubqueryAlias a\n:              +- SubqueryAlias spark_catalog.silver.tb_access\n:                 +- Relation spark_catalog.silver.tb_access[access_timestamp#69406,access_ip_address#69407,local_access#69408,access_uuid#69409,user_uuid#69410,course_uuid#69411,dt_load#69412] parquet\n+- 'Project ['dcal.sk_tempo, 'dcod.sk_cod, 'dc.sk_course, 'du.sk_user, 'ofs.discount_value, 'ofs.total_value]\n   +- 'Join Inner, (user_uuid#69464 = 'ofs.user_uuid)\n      :- Join Inner, (course_uuid#69458 = course_uuid#68126)\n      :  :- Join Inner, (date#69442 = cast(dt_sale#68123 as date))\n      :  :  :- Join Inner, (((payment_method#68127 = payment_method#69438) AND (user_origin#69382 = user_origin#69436)) AND ((access_local#69381 = access_from#69437) AND (percent_discount#68131 = percent_discount#69439)))\n      :  :  :  :- SubqueryAlias ofs\n      :  :  :  :  +- SubqueryAlias orig_fact_sales\n      :  :  :  :     +- CTERelationRef 7, true, [percent_discount#68131, access_local#69381, payment_method#68127, user_origin#69382, discount_value#68132, total_value#68130, course_uuid#68126, dt_sale#68123]\n      :  :  :  +- SubqueryAlias dcod\n      :  :  :     +- SubqueryAlias spark_catalog.gold.dim_cod\n      :  :  :        +- Relation spark_catalog.gold.dim_cod[sk_cod#69435L,user_origin#69436,access_from#69437,payment_method#69438,percent_discount#69439,dt_load#69440] parquet\n      :  :  +- SubqueryAlias dcal\n      :  :     +- SubqueryAlias spark_catalog.gold.dim_calendar\n      :  :        +- Relation spark_catalog.gold.dim_calendar[sk_tempo#69441L,date#69442,year#69443,month#69444,month_year#69445,day_week_int#69446,day_week#69447,fl_day_week#69448,day_month#69449,fl_last_month_day#69450,day_year#69451,week_year#69452,bimonthly#69453,quarter#69454,semester#69455,dt_load#69456] parquet\n      :  +- SubqueryAlias dc\n      :     +- SubqueryAlias spark_catalog.gold.dim_course\n      :        +- Relation spark_catalog.gold.dim_course[sk_course#69457L,course_uuid#69458,course_name#69459,course_level#69460,course_price#69461,dt_load#69462] parquet\n      +- SubqueryAlias du\n         +- SubqueryAlias spark_catalog.gold.dim_user\n            +- Relation spark_catalog.gold.dim_user[sk_user#69463L,user_uuid#69464,user_name#69465,user_email#69466,user_idade#69467,user_gender#69468,user_state#69469,user_profession#69470,company#69471,dt_load#69472] parquet\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `ofs`.`user_uuid` cannot be resolved. Did you mean one of the following? [`du`.`user_uuid`, `ofs`.`course_uuid`, `ofs`.`user_origin`, `dc`.`course_uuid`, `du`.`user_email`].; line 29 pos 47;\n'WithCTE\n:- ~CTERelationDef 7, false\n:  +- ~SubqueryAlias orig_fact_sales\n:     +- ~Project [percent_discount#68131, CASE WHEN (origin#68133 = FILE) THEN not applicable ELSE local_access#69408 END AS access_local#69381, payment_method#68127, origin#68133 AS user_origin#69382, discount_value#68132, total_value#68130, course_uuid#68126, dt_sale#68123]\n:        +- ~Join LeftOuter, (access_uuid#68124 = access_uuid#69409)\n:           :- ~SubqueryAlias s\n:           :  +- ~SubqueryAlias vw_fact_sales\n:           :     +- View (`vw_fact_sales`, [dt_sale#68123,access_uuid#68124,user_uuid#68125,course_uuid#68126,payment_method#68127,qnt_instalments#68128,instalments_values#68129,total_value#68130,percent_discount#68131,discount_value#68132,origin#68133,dt_load#68134])\n:           :        +- ~SubqueryAlias spark_catalog.silver.tb_sales\n:           :           +- ~StreamingRelation DataSource(org.apache.spark.sql.SparkSession@38ab048a,delta,List(),None,List(),None,Map(path -> *********(redacted)),Some(CatalogTable(\nCatalog: spark_catalog\nDatabase: silver\nTable: tb_sales\nOwner: root\nCreated Time: Thu Nov 28 00:54:05 UTC 2024\nLast Access: UNKNOWN\nCreated By: Spark 3.4.1\nType: MANAGED\nProvider: delta\nTable Properties: [delta.lastCommitTimestamp=1732755244000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\nLocation: dbfs:/user/hive/warehouse/silver.db/tb_sales\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog\nSchema: root\n |-- dt_sale: timestamp (nullable = true)\n |-- access_uuid: string (nullable = true)\n |-- user_uuid: string (nullable = true)\n |-- course_uuid: string (nullable = true)\n |-- payment_method: string (nullable = true)\n |-- qnt_instalments: integer (nullable = true)\n |-- instalments_values: decimal(9,2) (nullable = true)\n |-- total_value: decimal(9,2) (nullable = true)\n |-- percent_discount: string (nullable = true)\n |-- discount_value: decimal(9,2) (nullable = true)\n |-- origin: string (nullable = true)\n |-- dt_load: timestamp (nullable = true)\n))), tahoe, [dt_sale#68123, access_uuid#68124, user_uuid#68125, course_uuid#68126, payment_method#68127, qnt_instalments#68128, instalments_values#68129, total_value#68130, percent_discount#68131, discount_value#68132, origin#68133, dt_load#68134]\n:           +- SubqueryAlias a\n:              +- SubqueryAlias spark_catalog.silver.tb_access\n:                 +- Relation spark_catalog.silver.tb_access[access_timestamp#69406,access_ip_address#69407,local_access#69408,access_uuid#69409,user_uuid#69410,course_uuid#69411,dt_load#69412] parquet\n+- 'Project ['dcal.sk_tempo, 'dcod.sk_cod, 'dc.sk_course, 'du.sk_user, 'ofs.discount_value, 'ofs.total_value]\n   +- 'Join Inner, (user_uuid#69464 = 'ofs.user_uuid)\n      :- Join Inner, (course_uuid#69458 = course_uuid#68126)\n      :  :- Join Inner, (date#69442 = cast(dt_sale#68123 as date))\n      :  :  :- Join Inner, (((payment_method#68127 = payment_method#69438) AND (user_origin#69382 = user_origin#69436)) AND ((access_local#69381 = access_from#69437) AND (percent_discount#68131 = percent_discount#69439)))\n      :  :  :  :- SubqueryAlias ofs\n      :  :  :  :  +- SubqueryAlias orig_fact_sales\n      :  :  :  :     +- CTERelationRef 7, true, [percent_discount#68131, access_local#69381, payment_method#68127, user_origin#69382, discount_value#68132, total_value#68130, course_uuid#68126, dt_sale#68123]\n      :  :  :  +- SubqueryAlias dcod\n      :  :  :     +- SubqueryAlias spark_catalog.gold.dim_cod\n      :  :  :        +- Relation spark_catalog.gold.dim_cod[sk_cod#69435L,user_origin#69436,access_from#69437,payment_method#69438,percent_discount#69439,dt_load#69440] parquet\n      :  :  +- SubqueryAlias dcal\n      :  :     +- SubqueryAlias spark_catalog.gold.dim_calendar\n      :  :        +- Relation spark_catalog.gold.dim_calendar[sk_tempo#69441L,date#69442,year#69443,month#69444,month_year#69445,day_week_int#69446,day_week#69447,fl_day_week#69448,day_month#69449,fl_last_month_day#69450,day_year#69451,week_year#69452,bimonthly#69453,quarter#69454,semester#69455,dt_load#69456] parquet\n      :  +- SubqueryAlias dc\n      :     +- SubqueryAlias spark_catalog.gold.dim_course\n      :        +- Relation spark_catalog.gold.dim_course[sk_course#69457L,course_uuid#69458,course_name#69459,course_level#69460,course_price#69461,dt_load#69462] parquet\n      +- SubqueryAlias du\n         +- SubqueryAlias spark_catalog.gold.dim_user\n            +- Relation spark_catalog.gold.dim_user[sk_user#69463L,user_uuid#69464,user_name#69465,user_email#69466,user_idade#69467,user_gender#69468,user_state#69469,user_profession#69470,company#69471,dt_load#69472] parquet\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_fact_sales = spark.sql(\"\"\"\n",
    "    with orig_fact_sales (\n",
    "        select \n",
    "            s.percent_discount,\n",
    "            case when s.origin = 'FILE' then 'not applicable' else a.local_access end as access_local,\n",
    "            s.payment_method,\n",
    "            s.origin as user_origin,\n",
    "            s.discount_value,\n",
    "            s.total_value,\n",
    "            s.course_uuid,\n",
    "            s.dt_sale\n",
    "        from vw_fact_sales as s\n",
    "        left join silver.tb_access as a on s.access_uuid = a.access_uuid\n",
    "    )    \n",
    "    select   \n",
    "        dcal.sk_tempo,\n",
    "        dcod.sk_cod,\n",
    "        dc.sk_course,\n",
    "        du.sk_user,\n",
    "        ofs.discount_value,\n",
    "        ofs.total_value\n",
    "    from orig_fact_sales as ofs\n",
    "    join gold.dim_cod as dcod on ofs.payment_method = dcod.payment_method\n",
    "                                and ofs.user_origin = dcod.user_origin\n",
    "                                and ofs.access_local =dcod.access_from\n",
    "                                and ofs.percent_discount = dcod.percent_discount\n",
    "    join gold.dim_calendar as dcal on dcal.date = cast(ofs.dt_sale as date)\n",
    "    join gold.dim_course as dc on dc.course_uuid = ofs.course_uuid\n",
    "    join gold.dim_user as du on du.user_uuid = ofs.user_uuid\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "df_fact_sales.limit(5).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1672494714034817,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Load_Gold_Zone",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
