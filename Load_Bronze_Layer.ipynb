{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4720f721-0b11-426c-9027-25d17543ef20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Loading Data To Bronze Layer \n",
    "\n",
    "1. The origin of the table dados_arquivo are files and will be loaded in batch using spark. \n",
    "2. The origin of the table dados_api is the sales API and will be loaded in streamming using AutoLoader.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e63e37f-345e-49c9-9ce7-cf8d3c677517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Batch Data : Sales from sales consultant - B2B\n",
    "* Streaming Data : API Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e886810-a08e-43d4-86bb-95e290e8a9ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.0 Initial Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "057a83a2-36e3-471c-bef6-841781186a51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/cabreirajm@gmail.com/DataPipelineCabreira/Helpers/data_generator\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6f1d323-316d-48d4-8ba8-9e48d8549ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Data\n",
    "1. **generate_api_data** - This function generate the Sales API streaming data which is in JSON format and is stored in **Landing Zone** : `dbfs:/FileStore/landing/stream/`\n",
    "\n",
    "\n",
    "* The payload struct has information regarding user registration, the sale itself and the payment method. If an user just visit the website without any other action ( registration and selling), all elements will be set as null. \n",
    "\n",
    "An API file example is presented below : \n",
    "\n",
    "```\n",
    "    {\n",
    "        \"access_date\":\"2024-06-02T19:01:09.000Z\",\n",
    "        \"ip_address\":\"207.198.60.166\",\n",
    "        \"access_point\":\"chrome\",\n",
    "        \"payload\":{\n",
    "            \"info_usuario\":{\n",
    "                \"nome\":\"Usuario c3e5d305e1\",\n",
    "                \"idade\":\"44\",\n",
    "                \"sexo\":\"F\",\n",
    "                \"email\":\"usuario_c3e5d305e1@outlook.com\",\n",
    "                \"profissao\":\"Desenvolvedor de ETL\",\n",
    "                \"estado\":\"TO\"\n",
    "            },\n",
    "            \"info_produto\":{\n",
    "                \"product_uuid\":\"f260cd97c6c9813b01601e834a2added\",\n",
    "                \"valor\":\"R$ 589,90\"\n",
    "            },\n",
    "            \"info_pagamento\":{\n",
    "                \"valor\":\"589.90\",\n",
    "                \"forma_pagamento\":\"credito\",\n",
    "                \"quantidade_parcelas\":\"2\",\n",
    "                \"valor_parcelas\":\"294.95\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "2. **generate_files_data** - This function generates the batch data in csv format. As mentioned above, these csv data are data from sales consultant that sells the products to business ( B2B ). All the data are stored in **Landing Zone** `dbfs:/FileStore/landing/files/`.\n",
    "\n",
    "A batch file example is presented below:\n",
    "\n",
    "```\n",
    "    data_venda,nome_empresa,sexo,nome_funcionario,email_functionario,profissao,idade,estado,curso,valor,disconto\n",
    "    2025-02-26,Empresa A,M,Funcionario 3a0f99b401,funcionario_3a0f99b401@empresaa.com.br,Cientista de Dados,44,RO,Construindo o seu Primeiro Pipeline de Dados com o Databricks,\"R$ 789,90\",5%\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f2b0562-413d-446d-bf12-167636566a3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_api_data-inicio-2024-11-20 14:21:05.553818\ngenerate_api_data-fim-2024-11-20 14:21:13.186053\n              \ngenerate_files_data-inicio-2024-11-20 14:21:13.186325\nO arquivo .csv com 100000 registros foi gerado no diretorio 'dbfs:/FileStore/landing/files'.\ngenerate_files_data-fim-2024-11-20 14:21:42.492506\n              \n"
     ]
    }
   ],
   "source": [
    "# generate api data\n",
    "query = generate_api_data()\n",
    "\n",
    "# generate file data\n",
    "generate_files_data(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d11dc89f-a478-4abe-bc88-9f88264d999d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.0 Create Bronze Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24808ed4-343d-4ab4-92c4-61e260f4c594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85ec9018-2b98-4b79-8835-4b83d0bc1780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.0 Streaming Data Ingestion to Bronze Layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fdf77d1-fc71-446b-ad3d-fe8d52a9d25f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Using the method **spark.readStream** and the AutoLoader **.format('cloudFiles')** we will be able to read the API streming data with no need of a pre defined schema.\n",
    "\n",
    "* `option('cloudFiles.format', 'json')` : Allows us to read the json data with no need of defining the schema. It is used to identify the file format the Autoloader will process. \n",
    "* `option('cloudFiles.schemaLocation', schema_location_api))`: The local where the infered schema will be stored and versioned.\n",
    "* `option('cloudFiles.inferColumnTypes', True)`: Allows AutoLoader to infer the schema of all columns.\n",
    "* `load(stream_lading_path)`: Creates the readStream processes pointing to the local where the API is sending the streaming data ( the origin path)\n",
    "\n",
    "The `auto_loader_df` dataframe is created below following all the above requirements. In addition, we create two new columns : \n",
    "1. `source_file_name`: Indicates the identifier of the file that originated the registri. \n",
    "2. `processing_timestamp`: The timestamp when the data is ingested and processed. \n",
    "\n",
    "Note thar these two columns are added in order to help in possible debuggings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7029196e-fefa-46a6-94e8-bbcc56fc9409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3789230903044409>, line 14\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m stream_lading_path  \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/landing/stream/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      4\u001B[0m auto_loader_df \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m      5\u001B[0m   spark\u001B[38;5;241m.\u001B[39mreadStream\n",
       "\u001B[1;32m      6\u001B[0m        \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcloudFiles\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     10\u001B[0m        \u001B[38;5;241m.\u001B[39mload(stream_lading_path)\n",
       "\u001B[1;32m     11\u001B[0m )\n",
       "\u001B[1;32m     13\u001B[0m auto_loader_df \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[0;32m---> 14\u001B[0m   auto_loader_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_file_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_metadata.file_name\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m     15\u001B[0m                 \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprocessing_timestamp\u001B[39m\u001B[38;5;124m\"\u001B[39m, current_timestamp() )\n",
       "\u001B[1;32m     16\u001B[0m )\n",
       "\u001B[1;32m     18\u001B[0m auto_loader_df\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m5\u001B[39m)\u001B[38;5;241m.\u001B[39mdisplay()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'col' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-3789230903044409>, line 14\u001B[0m\n\u001B[1;32m      3\u001B[0m stream_lading_path  \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/landing/stream/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      4\u001B[0m auto_loader_df \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      5\u001B[0m   spark\u001B[38;5;241m.\u001B[39mreadStream\n\u001B[1;32m      6\u001B[0m        \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcloudFiles\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     10\u001B[0m        \u001B[38;5;241m.\u001B[39mload(stream_lading_path)\n\u001B[1;32m     11\u001B[0m )\n\u001B[1;32m     13\u001B[0m auto_loader_df \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m---> 14\u001B[0m   auto_loader_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_file_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_metadata.file_name\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     15\u001B[0m                 \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprocessing_timestamp\u001B[39m\u001B[38;5;124m\"\u001B[39m, current_timestamp() )\n\u001B[1;32m     16\u001B[0m )\n\u001B[1;32m     18\u001B[0m auto_loader_df\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m5\u001B[39m)\u001B[38;5;241m.\u001B[39mdisplay()\n\n\u001B[0;31mNameError\u001B[0m: name 'col' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'col' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Reading the API streaming data\n",
    "schema_location_api = 'dbfs:/user/hive/warehouse/bronze.db/_schemas/load_api_raw_data'\n",
    "stream_lading_path  = \"dbfs:/FileStore/landing/stream/\"\n",
    "auto_loader_df = (\n",
    "  spark.readStream\n",
    "       .format('cloudFiles')\n",
    "       .option('cloudFiles.format','json')\n",
    "       .option('cloudFiles.schemaLocation', schema_location_api)\n",
    "       .option('cloudFiles.inferColumnTypes', True)\n",
    "       .load(stream_lading_path)\n",
    ")\n",
    "\n",
    "auto_loader_df = (\n",
    "  auto_loader_df.withColumn(\"source_file_name\", col(\"_metadata.file_name\"))\n",
    "                .withColumn(\"processing_timestamp\", current_timestamp() )\n",
    ")\n",
    "\n",
    "auto_loader_df.limit(5).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd70e4c8-88b5-4cd9-b90d-80c3f47b40cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* The `_rescued_data` column contains data that spark couldnt identify during the process of schema evolution. In other words, the autoLoader stores within this column the registers that it was not able to identify its schema.\n",
    "* Example: Changes in data type might be stored since the autolader may not identify the schema evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f77a1f-b09f-4a17-8c19-a50db7b733e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_all_streams-inicio-2024-11-20 14:32:34.089135\nO stream generate_api_stream_data fui finalizado com sucesso.\nstop_all_streams-fim-2024-11-20 14:32:34.878001\n              \nclean_up_landing_dir-inicio-2024-11-20 14:32:34.878151\nTodos os arquivos e diretórios dentro de 'dbfs:/FileStore/landing/' foram excluidos com sucesso.\nclean_up_landing_dir-fim-2024-11-20 14:32:38.731395\n              \n"
     ]
    }
   ],
   "source": [
    "stop_all_streams()\n",
    "clean_up_landing_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59f02adc-36aa-49f2-b97d-b6939177831a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Load_Bronze_Layer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
